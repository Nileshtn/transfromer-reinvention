{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token based on ascii (depricated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input         : hi there!\n",
      "token         : [104, 105, 32, 116, 104, 101, 114, 101, 33]\n",
      "decoded_token : hi there!\n"
     ]
    }
   ],
   "source": [
    "#we wont be using this as we use the spm\n",
    "encoder = lambda input: [ord(char) for char in input]\n",
    "decoder = lambda input: ''.join([chr(char) for char in input])\n",
    "sample_input = \"hi there!\"\n",
    "token = encoder(sample_input)\n",
    "print(f\"input         : {sample_input}\\n\"\n",
    "      f\"token         : {token}\\n\"\n",
    "      f\"decoded_token : {decoder(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Sentencepiece based tokens~~ (tested the model is not good with tokenizer)\n",
    "- need to use the tokenizer_training note book to generate vcal lib and a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a vocab lib of 10000\n",
    "# sp = spm.SentencePieceProcessor(model_file='token.model')\n",
    "# print(sp.EncodeAsPieces(\"hello how are you?\"))\n",
    "# print(sp.tokenize(\"hello how are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [95, 113, 1], 'attention_mask': [1, 1, 1]}\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "#gpt2 was good   google/byt5-small\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "text = r\"\\n\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)  # Might retain \\n explicitly\n",
    "\n",
    "print(tokenizer.decode(113))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading the input and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length : 1003854\n",
      "val data length : 111540\n",
      "vocab lib size:  256\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/data.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "##uncommand to work with the ascii\n",
    "##converting the word into token and creating a tensor of it\n",
    "# token_data = torch.tensor(encoder(data))\n",
    "\n",
    "token_data = torch.tensor(tokenizer(data)['input_ids'])\n",
    "vocab_n = tokenizer.vocab_size\n",
    "#spliting data into train and val\n",
    "train_len = int(0.9 * len(token_data))\n",
    "train_data = token_data[:train_len]\n",
    "val_data = token_data[train_len:]\n",
    "\n",
    "print(f\"train data length : {train_data.numel()}\")\n",
    "print(f\"val data length : {val_data.numel()}\")\n",
    "\n",
    "\n",
    "print(\"vocab lib size: \",vocab_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the input and traget, with the batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5672)\n",
    "def get_batch(batch, block_len, data):\n",
    "    random_pos = torch.randint(0, len(data)-block_len, (batch,))\n",
    "    input = torch.stack([data[i:i+block_len] for i in random_pos])\n",
    "    target = torch.stack([data[i+1:i+block_len+1] for i in random_pos])\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "input : \n",
      "[[100 110 104  13  82 113 111 124]\n",
      " [105  35  76  35 101 104  35 118]\n",
      " [ 35 111 114 117 103  35 100 113]\n",
      " [ 13  86 107 100 111 111  47  35]]\n",
      "output: \n",
      "[[110 104  13  82 113 111 124  35]\n",
      " [ 35  76  35 101 104  35 118 114]\n",
      " [111 114 117 103  35 100 113 103]\n",
      " [ 86 107 100 111 111  47  35 122]]\n"
     ]
    }
   ],
   "source": [
    "batch = 4\n",
    "block_len = 8\n",
    "ipt, tgt = get_batch(batch, block_len, train_data)\n",
    "print(ipt.shape)\n",
    "print(f\"input : \\n{ipt.numpy()}\\n\"\n",
    "      f\"output: \\n{tgt.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, n_vocal=128, context_len=50):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(n_vocal, embedding_dim) # (vocab_size, embedding_dim)\n",
    "        self.time_embedding = nn.Parameter(torch.randn((embedding_dim, context_len))) # (embedding_dim, context_len)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, context_len = x.shape\n",
    "        time_range = torch.arange(context_len, device=x.device)\n",
    "        time_embed = self.time_embedding[:, time_range].unsqueeze(0).expand(batch, -1, -1)\n",
    "        token_embed = self.token_embedding(x)\n",
    "        teken_embed = self.dropout(token_embed)\n",
    "        return time_embed.permute(0, 2, 1) + token_embed  # (batch, context_len, embedding_dim)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, context_len=50, embedding_dim=64, attention_dim=8, mode=\"encoder\"):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.mode = mode\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_len, context_len)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape  \n",
    "        q = self.query(x)  \n",
    "        k = self.key(x)    \n",
    "        v = self.value(x)  \n",
    "        d_k = q.shape[-1]\n",
    "        scaled_scores = torch.bmm(q, k.permute(0, 2, 1)) / (d_k ** 0.5)\n",
    "\n",
    "        if self.mode == \"encoder\":\n",
    "            attn_weights = scaled_scores\n",
    "        else:\n",
    "            attn_weights = scaled_scores.masked_fill(self.tril[:t, :t] == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attention_output = torch.bmm(attn_weights, v)  \n",
    "        return attention_output  \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, context_l=50, embedding_dim=64, heads=8, mode=\"decoder\"):\n",
    "        super().__init__()\n",
    "        self.attention_dim = embedding_dim // heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(context_len=context_l, embedding_dim=embedding_dim, attention_dim=self.attention_dim, mode=mode)\n",
    "            for _ in range(heads)\n",
    "        ])\n",
    "        self.projection_weight = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        x_ln = F.layer_norm(x, (c,))  # Correct normalization\n",
    "        output = [head(x_ln) for head in self.heads]\n",
    "        output = torch.cat(output, dim=2)\n",
    "        x_p = self.projection_weight(output)\n",
    "        x_p = self.dropout(x_p)\n",
    "        return x + x_p\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, mlp_multiplier=4):\n",
    "        super().__init__()\n",
    "        self.mlp_weight = nn.Linear(embedding_dim, embedding_dim * mlp_multiplier)\n",
    "        self.mlp_projection = nn.Linear(embedding_dim * mlp_multiplier, embedding_dim)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        x_ln = F.layer_norm(x, (c,))\n",
    "        output = self.gelu1(self.mlp_weight(x_ln))\n",
    "        output = self.dropout(output)\n",
    "        output = self.mlp_projection(output)\n",
    "        return x + output\n",
    "\n",
    "class Logits(nn.Module):\n",
    "    def __init__(self, n_vocal=128, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encode = nn.Linear(embedding_dim, n_vocal, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        x_ln = F.layer_norm(x, (c,))  # Normalize across channels (embedding dim)\n",
    "        logits = self.encode(x_ln)  # (b, t, n_vocal)\n",
    "        return logits  # No softmax applied\n",
    "\n",
    "class TransformerMini(nn.Module):\n",
    "    def __init__(self, context_l=50, n_vocal=128, embedding_dim=64, attention_heads=8, mode=\"decoder\"):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(embedding_dim, n_vocal, context_l)\n",
    "        self.self_attention = SelfAttention(context_l, embedding_dim, attention_heads, mode)\n",
    "        self.mlp = MLP(embedding_dim)\n",
    "        self.logits = Logits(n_vocal, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.self_attention(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.logits(x)\n",
    "        return x  # Output shape: (batch, context_len, n_vocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerMini(\n",
       "  (embedding): Embedding(\n",
       "    (token_embedding): Embedding(256, 256)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (self_attention): SelfAttention(\n",
       "    (heads): ModuleList(\n",
       "      (0-31): 32 x Head(\n",
       "        (query): Linear(in_features=256, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=256, out_features=8, bias=False)\n",
       "        (key): Linear(in_features=256, out_features=8, bias=False)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (projection_weight): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp_weight): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (mlp_projection): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (gelu1): GELU(approximate='none')\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (logits): Logits(\n",
       "    (encode): Linear(in_features=256, out_features=256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = \"../models/transformer_trained_model/trained_v0.0.3.pth\"\n",
    "# pretrained_model = None\n",
    "context_lenth = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = TransformerMini(context_l=context_lenth, n_vocal=vocab_n, embedding_dim=256, attention_heads=32).cuda()\n",
    "if pretrained_model:\n",
    "    model.load_state_dict(torch.load(pretrained_model, weights_only=True), strict=False)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd=0\n",
    "batch = 64\n",
    "epoch = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 2.4586, Val Loss: 2.4232\n",
      "Epoch: 1, Train Loss: 2.4485, Val Loss: 2.4135\n",
      "Epoch: 2, Train Loss: 2.4588, Val Loss: 2.4402\n",
      "Epoch: 3, Train Loss: 2.4541, Val Loss: 2.4219\n",
      "Epoch: 4, Train Loss: 2.4507, Val Loss: 2.4166\n",
      "Epoch: 5, Train Loss: 2.4574, Val Loss: 2.4049\n",
      "Epoch: 6, Train Loss: 2.4357, Val Loss: 2.4313\n",
      "Epoch: 7, Train Loss: 2.4434, Val Loss: 2.4194\n",
      "Epoch: 8, Train Loss: 2.4567, Val Loss: 2.4000\n",
      "Epoch: 9, Train Loss: 2.4402, Val Loss: 2.4254\n",
      "Epoch: 10, Train Loss: 2.4333, Val Loss: 2.4473\n",
      "Epoch: 11, Train Loss: 2.4401, Val Loss: 2.4167\n",
      "Epoch: 12, Train Loss: 2.4559, Val Loss: 2.4417\n",
      "Epoch: 13, Train Loss: 2.4496, Val Loss: 2.4329\n",
      "Epoch: 14, Train Loss: 2.4569, Val Loss: 2.4310\n",
      "Epoch: 15, Train Loss: 2.4526, Val Loss: 2.4539\n",
      "Epoch: 16, Train Loss: 2.4407, Val Loss: 2.4575\n",
      "Epoch: 17, Train Loss: 2.4543, Val Loss: 2.4186\n",
      "Epoch: 18, Train Loss: 2.4409, Val Loss: 2.4135\n",
      "Epoch: 19, Train Loss: 2.4379, Val Loss: 2.4057\n",
      "Epoch: 20, Train Loss: 2.4360, Val Loss: 2.4119\n",
      "Epoch: 21, Train Loss: 2.4492, Val Loss: 2.4526\n",
      "Epoch: 22, Train Loss: 2.4439, Val Loss: 2.4206\n",
      "Epoch: 23, Train Loss: 2.4481, Val Loss: 2.4255\n",
      "Epoch: 24, Train Loss: 2.4327, Val Loss: 2.4023\n",
      "Epoch: 25, Train Loss: 2.4615, Val Loss: 2.4000\n",
      "Epoch: 26, Train Loss: 2.4483, Val Loss: 2.4031\n",
      "Epoch: 27, Train Loss: 2.4482, Val Loss: 2.4395\n",
      "Epoch: 28, Train Loss: 2.4296, Val Loss: 2.4234\n",
      "Epoch: 29, Train Loss: 2.4439, Val Loss: 2.4132\n",
      "Epoch: 30, Train Loss: 2.4307, Val Loss: 2.3949\n",
      "Epoch: 31, Train Loss: 2.4428, Val Loss: 2.4473\n",
      "Epoch: 32, Train Loss: 2.4422, Val Loss: 2.4355\n",
      "Epoch: 33, Train Loss: 2.4448, Val Loss: 2.4623\n",
      "Epoch: 34, Train Loss: 2.4563, Val Loss: 2.4132\n",
      "Epoch: 35, Train Loss: 2.4408, Val Loss: 2.4237\n",
      "Epoch: 36, Train Loss: 2.4530, Val Loss: 2.4051\n",
      "Epoch: 37, Train Loss: 2.4390, Val Loss: 2.4220\n",
      "Epoch: 38, Train Loss: 2.4598, Val Loss: 2.4271\n",
      "Epoch: 39, Train Loss: 2.4506, Val Loss: 2.3812\n",
      "Epoch: 40, Train Loss: 2.4486, Val Loss: 2.4168\n",
      "Epoch: 41, Train Loss: 2.4444, Val Loss: 2.4231\n",
      "Epoch: 42, Train Loss: 2.4532, Val Loss: 2.4329\n",
      "Epoch: 43, Train Loss: 2.4520, Val Loss: 2.3962\n",
      "Epoch: 44, Train Loss: 2.4385, Val Loss: 2.4178\n",
      "Epoch: 45, Train Loss: 2.4358, Val Loss: 2.4281\n",
      "Epoch: 46, Train Loss: 2.4429, Val Loss: 2.4056\n",
      "Epoch: 47, Train Loss: 2.4364, Val Loss: 2.4121\n",
      "Epoch: 48, Train Loss: 2.4439, Val Loss: 2.3910\n",
      "Epoch: 49, Train Loss: 2.4387, Val Loss: 2.4042\n",
      "Epoch: 50, Train Loss: 2.4529, Val Loss: 2.4164\n",
      "Epoch: 51, Train Loss: 2.4326, Val Loss: 2.4137\n",
      "Epoch: 52, Train Loss: 2.4541, Val Loss: 2.3991\n",
      "Epoch: 53, Train Loss: 2.4553, Val Loss: 2.4144\n",
      "Epoch: 54, Train Loss: 2.4440, Val Loss: 2.4087\n",
      "Epoch: 55, Train Loss: 2.4564, Val Loss: 2.3942\n",
      "Epoch: 56, Train Loss: 2.4384, Val Loss: 2.3837\n",
      "Epoch: 57, Train Loss: 2.4509, Val Loss: 2.3858\n",
      "Epoch: 58, Train Loss: 2.4402, Val Loss: 2.4215\n",
      "Epoch: 59, Train Loss: 2.4479, Val Loss: 2.3968\n",
      "Epoch: 60, Train Loss: 2.4413, Val Loss: 2.4249\n",
      "Epoch: 61, Train Loss: 2.4572, Val Loss: 2.4086\n",
      "Epoch: 62, Train Loss: 2.4323, Val Loss: 2.3985\n",
      "Epoch: 63, Train Loss: 2.4476, Val Loss: 2.3779\n",
      "Epoch: 64, Train Loss: 2.4401, Val Loss: 2.4012\n",
      "Epoch: 65, Train Loss: 2.4552, Val Loss: 2.3967\n",
      "Epoch: 66, Train Loss: 2.4440, Val Loss: 2.4052\n",
      "Epoch: 67, Train Loss: 2.4312, Val Loss: 2.3910\n",
      "Epoch: 68, Train Loss: 2.4365, Val Loss: 2.4103\n",
      "Epoch: 69, Train Loss: 2.4331, Val Loss: 2.4006\n",
      "Epoch: 70, Train Loss: 2.4346, Val Loss: 2.4026\n",
      "Epoch: 71, Train Loss: 2.4376, Val Loss: 2.4079\n",
      "Epoch: 72, Train Loss: 2.4533, Val Loss: 2.3969\n",
      "Epoch: 73, Train Loss: 2.4537, Val Loss: 2.3929\n",
      "Epoch: 74, Train Loss: 2.4435, Val Loss: 2.3784\n",
      "Epoch: 75, Train Loss: 2.4418, Val Loss: 2.3952\n",
      "Epoch: 76, Train Loss: 2.4362, Val Loss: 2.4170\n",
      "Epoch: 77, Train Loss: 2.4394, Val Loss: 2.3933\n",
      "Epoch: 78, Train Loss: 2.4492, Val Loss: 2.4248\n",
      "Epoch: 79, Train Loss: 2.4541, Val Loss: 2.4088\n",
      "Epoch: 80, Train Loss: 2.4446, Val Loss: 2.3617\n",
      "Epoch: 81, Train Loss: 2.4382, Val Loss: 2.4156\n",
      "Epoch: 82, Train Loss: 2.4520, Val Loss: 2.3872\n",
      "Epoch: 83, Train Loss: 2.4496, Val Loss: 2.3901\n",
      "Epoch: 84, Train Loss: 2.4547, Val Loss: 2.3855\n",
      "Epoch: 85, Train Loss: 2.4340, Val Loss: 2.4126\n",
      "Epoch: 86, Train Loss: 2.4485, Val Loss: 2.3755\n",
      "Epoch: 87, Train Loss: 2.4336, Val Loss: 2.4180\n",
      "Epoch: 88, Train Loss: 2.4450, Val Loss: 2.4055\n",
      "Epoch: 89, Train Loss: 2.4415, Val Loss: 2.4246\n",
      "Epoch: 90, Train Loss: 2.4414, Val Loss: 2.3965\n",
      "Epoch: 91, Train Loss: 2.4397, Val Loss: 2.3860\n",
      "Epoch: 92, Train Loss: 2.4382, Val Loss: 2.3929\n",
      "Epoch: 93, Train Loss: 2.4505, Val Loss: 2.3851\n",
      "Epoch: 94, Train Loss: 2.4303, Val Loss: 2.3986\n",
      "Epoch: 95, Train Loss: 2.4314, Val Loss: 2.4301\n",
      "Epoch: 96, Train Loss: 2.4374, Val Loss: 2.4057\n",
      "Epoch: 97, Train Loss: 2.4353, Val Loss: 2.3938\n",
      "Epoch: 98, Train Loss: 2.4381, Val Loss: 2.4118\n",
      "Epoch: 99, Train Loss: 2.4301, Val Loss: 2.3620\n",
      "Epoch: 100, Train Loss: 2.4381, Val Loss: 2.3745\n",
      "Epoch: 101, Train Loss: 2.4512, Val Loss: 2.4138\n",
      "Epoch: 102, Train Loss: 2.4449, Val Loss: 2.3849\n",
      "Epoch: 103, Train Loss: 2.4504, Val Loss: 2.3635\n",
      "Epoch: 104, Train Loss: 2.4361, Val Loss: 2.3850\n",
      "Epoch: 105, Train Loss: 2.4494, Val Loss: 2.3962\n",
      "Epoch: 106, Train Loss: 2.4254, Val Loss: 2.4067\n",
      "Epoch: 107, Train Loss: 2.4307, Val Loss: 2.4118\n",
      "Epoch: 108, Train Loss: 2.4350, Val Loss: 2.3947\n",
      "Epoch: 109, Train Loss: 2.4445, Val Loss: 2.3777\n",
      "Epoch: 110, Train Loss: 2.4324, Val Loss: 2.3959\n",
      "Epoch: 111, Train Loss: 2.4287, Val Loss: 2.3705\n",
      "Epoch: 112, Train Loss: 2.4396, Val Loss: 2.3878\n",
      "Epoch: 113, Train Loss: 2.4213, Val Loss: 2.3730\n",
      "Epoch: 114, Train Loss: 2.4396, Val Loss: 2.4154\n",
      "Epoch: 115, Train Loss: 2.4295, Val Loss: 2.4012\n",
      "Epoch: 116, Train Loss: 2.4216, Val Loss: 2.3997\n",
      "Epoch: 117, Train Loss: 2.4234, Val Loss: 2.3635\n",
      "Epoch: 118, Train Loss: 2.4223, Val Loss: 2.3736\n",
      "Epoch: 119, Train Loss: 2.4330, Val Loss: 2.4001\n",
      "Epoch: 120, Train Loss: 2.4351, Val Loss: 2.3816\n",
      "Epoch: 121, Train Loss: 2.4205, Val Loss: 2.3931\n",
      "Epoch: 122, Train Loss: 2.4319, Val Loss: 2.3645\n",
      "Epoch: 123, Train Loss: 2.4308, Val Loss: 2.3639\n",
      "Epoch: 124, Train Loss: 2.4400, Val Loss: 2.4097\n",
      "Epoch: 125, Train Loss: 2.4442, Val Loss: 2.3734\n",
      "Epoch: 126, Train Loss: 2.4301, Val Loss: 2.3660\n",
      "Epoch: 127, Train Loss: 2.4338, Val Loss: 2.3959\n",
      "Epoch: 128, Train Loss: 2.4255, Val Loss: 2.3696\n",
      "Epoch: 129, Train Loss: 2.4239, Val Loss: 2.3823\n",
      "Epoch: 130, Train Loss: 2.4381, Val Loss: 2.3797\n",
      "Epoch: 131, Train Loss: 2.4330, Val Loss: 2.3959\n",
      "Epoch: 132, Train Loss: 2.4471, Val Loss: 2.3933\n",
      "Epoch: 133, Train Loss: 2.4318, Val Loss: 2.3667\n",
      "Epoch: 134, Train Loss: 2.4253, Val Loss: 2.3797\n",
      "Epoch: 135, Train Loss: 2.4168, Val Loss: 2.3703\n",
      "Epoch: 136, Train Loss: 2.4216, Val Loss: 2.3337\n",
      "Epoch: 137, Train Loss: 2.4256, Val Loss: 2.3683\n",
      "Epoch: 138, Train Loss: 2.4281, Val Loss: 2.3389\n",
      "Epoch: 139, Train Loss: 2.4474, Val Loss: 2.3981\n",
      "Epoch: 140, Train Loss: 2.4241, Val Loss: 2.3989\n",
      "Epoch: 141, Train Loss: 2.4232, Val Loss: 2.3811\n",
      "Epoch: 142, Train Loss: 2.4161, Val Loss: 2.4156\n",
      "Epoch: 143, Train Loss: 2.4344, Val Loss: 2.3875\n",
      "Epoch: 144, Train Loss: 2.4225, Val Loss: 2.3744\n",
      "Epoch: 145, Train Loss: 2.4322, Val Loss: 2.3676\n",
      "Epoch: 146, Train Loss: 2.4236, Val Loss: 2.3666\n",
      "Epoch: 147, Train Loss: 2.4317, Val Loss: 2.3771\n",
      "Epoch: 148, Train Loss: 2.4349, Val Loss: 2.3802\n",
      "Epoch: 149, Train Loss: 2.4307, Val Loss: 2.3701\n",
      "Epoch: 150, Train Loss: 2.4200, Val Loss: 2.3887\n",
      "Epoch: 151, Train Loss: 2.4365, Val Loss: 2.3667\n",
      "Epoch: 152, Train Loss: 2.4253, Val Loss: 2.3921\n",
      "Epoch: 153, Train Loss: 2.4301, Val Loss: 2.3675\n",
      "Epoch: 154, Train Loss: 2.4161, Val Loss: 2.3520\n",
      "Epoch: 155, Train Loss: 2.4128, Val Loss: 2.3649\n",
      "Epoch: 156, Train Loss: 2.4219, Val Loss: 2.3904\n",
      "Epoch: 157, Train Loss: 2.4205, Val Loss: 2.3740\n",
      "Epoch: 158, Train Loss: 2.4319, Val Loss: 2.3792\n",
      "Epoch: 159, Train Loss: 2.4288, Val Loss: 2.3699\n",
      "Epoch: 160, Train Loss: 2.4170, Val Loss: 2.3758\n",
      "Epoch: 161, Train Loss: 2.4249, Val Loss: 2.3599\n",
      "Epoch: 162, Train Loss: 2.4303, Val Loss: 2.3611\n",
      "Epoch: 163, Train Loss: 2.4215, Val Loss: 2.3838\n",
      "Epoch: 164, Train Loss: 2.4237, Val Loss: 2.3807\n",
      "Epoch: 165, Train Loss: 2.4218, Val Loss: 2.3815\n",
      "Epoch: 166, Train Loss: 2.4214, Val Loss: 2.3494\n",
      "Epoch: 167, Train Loss: 2.4240, Val Loss: 2.3667\n",
      "Epoch: 168, Train Loss: 2.4314, Val Loss: 2.3346\n",
      "Epoch: 169, Train Loss: 2.4273, Val Loss: 2.3445\n",
      "Epoch: 170, Train Loss: 2.4257, Val Loss: 2.3524\n",
      "Epoch: 171, Train Loss: 2.4501, Val Loss: 2.3623\n",
      "Epoch: 172, Train Loss: 2.4256, Val Loss: 2.3418\n",
      "Epoch: 173, Train Loss: 2.4140, Val Loss: 2.3443\n",
      "Epoch: 174, Train Loss: 2.4148, Val Loss: 2.3676\n",
      "Epoch: 175, Train Loss: 2.4082, Val Loss: 2.3490\n",
      "Epoch: 176, Train Loss: 2.4311, Val Loss: 2.3641\n",
      "Epoch: 177, Train Loss: 2.4202, Val Loss: 2.3545\n",
      "Epoch: 178, Train Loss: 2.4291, Val Loss: 2.3290\n",
      "Epoch: 179, Train Loss: 2.4162, Val Loss: 2.3275\n",
      "Epoch: 180, Train Loss: 2.4248, Val Loss: 2.3504\n",
      "Epoch: 181, Train Loss: 2.4117, Val Loss: 2.3447\n",
      "Epoch: 182, Train Loss: 2.4198, Val Loss: 2.3541\n",
      "Epoch: 183, Train Loss: 2.4158, Val Loss: 2.3466\n",
      "Epoch: 184, Train Loss: 2.4067, Val Loss: 2.3547\n",
      "Epoch: 185, Train Loss: 2.4050, Val Loss: 2.3578\n",
      "Epoch: 186, Train Loss: 2.4261, Val Loss: 2.3644\n",
      "Epoch: 187, Train Loss: 2.4198, Val Loss: 2.3490\n",
      "Epoch: 188, Train Loss: 2.4294, Val Loss: 2.3442\n",
      "Epoch: 189, Train Loss: 2.4169, Val Loss: 2.3611\n",
      "Epoch: 190, Train Loss: 2.4098, Val Loss: 2.3750\n",
      "Epoch: 191, Train Loss: 2.4225, Val Loss: 2.3511\n",
      "Epoch: 192, Train Loss: 2.4273, Val Loss: 2.3168\n",
      "Epoch: 193, Train Loss: 2.4095, Val Loss: 2.3116\n",
      "Epoch: 194, Train Loss: 2.4221, Val Loss: 2.3380\n",
      "Epoch: 195, Train Loss: 2.4188, Val Loss: 2.3613\n",
      "Epoch: 196, Train Loss: 2.4201, Val Loss: 2.3384\n",
      "Epoch: 197, Train Loss: 2.4217, Val Loss: 2.3354\n",
      "Epoch: 198, Train Loss: 2.4013, Val Loss: 2.3522\n",
      "Epoch: 199, Train Loss: 2.4192, Val Loss: 2.3504\n",
      "Epoch: 200, Train Loss: 2.4179, Val Loss: 2.3707\n",
      "Epoch: 201, Train Loss: 2.4112, Val Loss: 2.3257\n",
      "Epoch: 202, Train Loss: 2.4165, Val Loss: 2.3264\n",
      "Epoch: 203, Train Loss: 2.3949, Val Loss: 2.3383\n",
      "Epoch: 204, Train Loss: 2.4168, Val Loss: 2.3224\n",
      "Epoch: 205, Train Loss: 2.3937, Val Loss: 2.3489\n",
      "Epoch: 206, Train Loss: 2.4197, Val Loss: 2.3247\n",
      "Epoch: 207, Train Loss: 2.4193, Val Loss: 2.3506\n",
      "Epoch: 208, Train Loss: 2.3918, Val Loss: 2.3408\n",
      "Epoch: 209, Train Loss: 2.4052, Val Loss: 2.3228\n",
      "Epoch: 210, Train Loss: 2.4197, Val Loss: 2.3626\n",
      "Epoch: 211, Train Loss: 2.4180, Val Loss: 2.3256\n",
      "Epoch: 212, Train Loss: 2.4060, Val Loss: 2.3223\n",
      "Epoch: 213, Train Loss: 2.4131, Val Loss: 2.3387\n",
      "Epoch: 214, Train Loss: 2.4108, Val Loss: 2.3369\n",
      "Epoch: 215, Train Loss: 2.4095, Val Loss: 2.3285\n",
      "Epoch: 216, Train Loss: 2.4078, Val Loss: 2.3200\n",
      "Epoch: 217, Train Loss: 2.4052, Val Loss: 2.3207\n",
      "Epoch: 218, Train Loss: 2.4102, Val Loss: 2.3237\n",
      "Epoch: 219, Train Loss: 2.4168, Val Loss: 2.3305\n",
      "Epoch: 220, Train Loss: 2.4080, Val Loss: 2.3530\n",
      "Epoch: 221, Train Loss: 2.4016, Val Loss: 2.3348\n",
      "Epoch: 222, Train Loss: 2.4033, Val Loss: 2.3565\n",
      "Epoch: 223, Train Loss: 2.4001, Val Loss: 2.3275\n",
      "Epoch: 224, Train Loss: 2.4171, Val Loss: 2.3476\n",
      "Epoch: 225, Train Loss: 2.4015, Val Loss: 2.3204\n",
      "Epoch: 226, Train Loss: 2.4065, Val Loss: 2.3263\n",
      "Epoch: 227, Train Loss: 2.3962, Val Loss: 2.3250\n",
      "Epoch: 228, Train Loss: 2.4080, Val Loss: 2.3243\n",
      "Epoch: 229, Train Loss: 2.4261, Val Loss: 2.3467\n",
      "Epoch: 230, Train Loss: 2.4185, Val Loss: 2.3244\n",
      "Epoch: 231, Train Loss: 2.4012, Val Loss: 2.3346\n",
      "Epoch: 232, Train Loss: 2.4058, Val Loss: 2.3253\n",
      "Epoch: 233, Train Loss: 2.4144, Val Loss: 2.3313\n",
      "Epoch: 234, Train Loss: 2.4046, Val Loss: 2.3395\n",
      "Epoch: 235, Train Loss: 2.4181, Val Loss: 2.3274\n",
      "Epoch: 236, Train Loss: 2.4092, Val Loss: 2.3155\n",
      "Epoch: 237, Train Loss: 2.3997, Val Loss: 2.3357\n",
      "Epoch: 238, Train Loss: 2.3893, Val Loss: 2.3194\n",
      "Epoch: 239, Train Loss: 2.4067, Val Loss: 2.3051\n",
      "Epoch: 240, Train Loss: 2.3976, Val Loss: 2.3423\n",
      "Epoch: 241, Train Loss: 2.4035, Val Loss: 2.3332\n",
      "Epoch: 242, Train Loss: 2.3881, Val Loss: 2.3425\n",
      "Epoch: 243, Train Loss: 2.4089, Val Loss: 2.3199\n",
      "Epoch: 244, Train Loss: 2.4010, Val Loss: 2.3363\n",
      "Epoch: 245, Train Loss: 2.4048, Val Loss: 2.3024\n",
      "Epoch: 246, Train Loss: 2.3816, Val Loss: 2.3203\n",
      "Epoch: 247, Train Loss: 2.3950, Val Loss: 2.3565\n",
      "Epoch: 248, Train Loss: 2.4002, Val Loss: 2.3488\n",
      "Epoch: 249, Train Loss: 2.4094, Val Loss: 2.3083\n",
      "Epoch: 250, Train Loss: 2.3958, Val Loss: 2.3236\n",
      "Epoch: 251, Train Loss: 2.3969, Val Loss: 2.3169\n",
      "Epoch: 252, Train Loss: 2.3899, Val Loss: 2.3052\n",
      "Epoch: 253, Train Loss: 2.3950, Val Loss: 2.3012\n",
      "Epoch: 254, Train Loss: 2.4067, Val Loss: 2.3175\n",
      "Epoch: 255, Train Loss: 2.3870, Val Loss: 2.3389\n",
      "Epoch: 256, Train Loss: 2.3923, Val Loss: 2.3428\n",
      "Epoch: 257, Train Loss: 2.3971, Val Loss: 2.3248\n",
      "Epoch: 258, Train Loss: 2.3964, Val Loss: 2.3141\n",
      "Epoch: 259, Train Loss: 2.3869, Val Loss: 2.2963\n",
      "Epoch: 260, Train Loss: 2.4187, Val Loss: 2.3052\n",
      "Epoch: 261, Train Loss: 2.3788, Val Loss: 2.3232\n",
      "Epoch: 262, Train Loss: 2.4106, Val Loss: 2.3442\n",
      "Epoch: 263, Train Loss: 2.3844, Val Loss: 2.3306\n",
      "Epoch: 264, Train Loss: 2.3956, Val Loss: 2.2812\n",
      "Epoch: 265, Train Loss: 2.3893, Val Loss: 2.3079\n",
      "Epoch: 266, Train Loss: 2.3995, Val Loss: 2.3049\n",
      "Epoch: 267, Train Loss: 2.4105, Val Loss: 2.2760\n",
      "Epoch: 268, Train Loss: 2.4053, Val Loss: 2.3342\n",
      "Epoch: 269, Train Loss: 2.3910, Val Loss: 2.3090\n",
      "Epoch: 270, Train Loss: 2.3946, Val Loss: 2.3276\n",
      "Epoch: 271, Train Loss: 2.3873, Val Loss: 2.3063\n",
      "Epoch: 272, Train Loss: 2.3915, Val Loss: 2.3153\n",
      "Epoch: 273, Train Loss: 2.3812, Val Loss: 2.3460\n",
      "Epoch: 274, Train Loss: 2.3954, Val Loss: 2.3197\n",
      "Epoch: 275, Train Loss: 2.3910, Val Loss: 2.3241\n",
      "Epoch: 276, Train Loss: 2.3964, Val Loss: 2.3114\n",
      "Epoch: 277, Train Loss: 2.3909, Val Loss: 2.3195\n",
      "Epoch: 278, Train Loss: 2.3902, Val Loss: 2.2914\n",
      "Epoch: 279, Train Loss: 2.3915, Val Loss: 2.2985\n",
      "Epoch: 280, Train Loss: 2.3723, Val Loss: 2.2674\n",
      "Epoch: 281, Train Loss: 2.3794, Val Loss: 2.2842\n",
      "Epoch: 282, Train Loss: 2.3770, Val Loss: 2.2828\n",
      "Epoch: 283, Train Loss: 2.3931, Val Loss: 2.3050\n",
      "Epoch: 284, Train Loss: 2.3860, Val Loss: 2.2833\n",
      "Epoch: 285, Train Loss: 2.3833, Val Loss: 2.2731\n",
      "Epoch: 286, Train Loss: 2.3715, Val Loss: 2.2657\n",
      "Epoch: 287, Train Loss: 2.3902, Val Loss: 2.2877\n",
      "Epoch: 288, Train Loss: 2.3880, Val Loss: 2.3265\n",
      "Epoch: 289, Train Loss: 2.3912, Val Loss: 2.2764\n",
      "Epoch: 290, Train Loss: 2.3854, Val Loss: 2.2814\n",
      "Epoch: 291, Train Loss: 2.3793, Val Loss: 2.3253\n",
      "Epoch: 292, Train Loss: 2.3930, Val Loss: 2.3029\n",
      "Epoch: 293, Train Loss: 2.3870, Val Loss: 2.2797\n",
      "Epoch: 294, Train Loss: 2.3685, Val Loss: 2.2934\n",
      "Epoch: 295, Train Loss: 2.3886, Val Loss: 2.3365\n",
      "Epoch: 296, Train Loss: 2.3820, Val Loss: 2.2910\n",
      "Epoch: 297, Train Loss: 2.3839, Val Loss: 2.2977\n",
      "Epoch: 298, Train Loss: 2.3828, Val Loss: 2.2694\n",
      "Epoch: 299, Train Loss: 2.3836, Val Loss: 2.2803\n",
      "Epoch: 300, Train Loss: 2.3823, Val Loss: 2.2617\n",
      "Epoch: 301, Train Loss: 2.3867, Val Loss: 2.3029\n",
      "Epoch: 302, Train Loss: 2.3776, Val Loss: 2.2870\n",
      "Epoch: 303, Train Loss: 2.3781, Val Loss: 2.2827\n",
      "Epoch: 304, Train Loss: 2.3799, Val Loss: 2.2587\n",
      "Epoch: 305, Train Loss: 2.3886, Val Loss: 2.3157\n",
      "Epoch: 306, Train Loss: 2.3783, Val Loss: 2.2713\n",
      "Epoch: 307, Train Loss: 2.3770, Val Loss: 2.3178\n",
      "Epoch: 308, Train Loss: 2.3889, Val Loss: 2.2902\n",
      "Epoch: 309, Train Loss: 2.3624, Val Loss: 2.3092\n",
      "Epoch: 310, Train Loss: 2.3751, Val Loss: 2.3068\n",
      "Epoch: 311, Train Loss: 2.3821, Val Loss: 2.2897\n",
      "Epoch: 312, Train Loss: 2.3714, Val Loss: 2.2429\n",
      "Epoch: 313, Train Loss: 2.3616, Val Loss: 2.2763\n",
      "Epoch: 314, Train Loss: 2.3779, Val Loss: 2.3030\n",
      "Epoch: 315, Train Loss: 2.3599, Val Loss: 2.3088\n",
      "Epoch: 316, Train Loss: 2.3808, Val Loss: 2.2978\n",
      "Epoch: 317, Train Loss: 2.3808, Val Loss: 2.2601\n",
      "Epoch: 318, Train Loss: 2.3680, Val Loss: 2.3003\n",
      "Epoch: 319, Train Loss: 2.3880, Val Loss: 2.2818\n",
      "Epoch: 320, Train Loss: 2.3785, Val Loss: 2.2969\n",
      "Epoch: 321, Train Loss: 2.3735, Val Loss: 2.2738\n",
      "Epoch: 322, Train Loss: 2.3586, Val Loss: 2.2853\n",
      "Epoch: 323, Train Loss: 2.3713, Val Loss: 2.3119\n",
      "Epoch: 324, Train Loss: 2.3533, Val Loss: 2.2912\n",
      "Epoch: 325, Train Loss: 2.3700, Val Loss: 2.2684\n",
      "Epoch: 326, Train Loss: 2.3764, Val Loss: 2.2885\n",
      "Epoch: 327, Train Loss: 2.3646, Val Loss: 2.3014\n",
      "Epoch: 328, Train Loss: 2.3651, Val Loss: 2.3013\n",
      "Epoch: 329, Train Loss: 2.3680, Val Loss: 2.2767\n",
      "Epoch: 330, Train Loss: 2.3659, Val Loss: 2.2719\n",
      "Epoch: 331, Train Loss: 2.3646, Val Loss: 2.2994\n",
      "Epoch: 332, Train Loss: 2.3634, Val Loss: 2.2734\n",
      "Epoch: 333, Train Loss: 2.3845, Val Loss: 2.2734\n",
      "Epoch: 334, Train Loss: 2.3814, Val Loss: 2.2687\n",
      "Epoch: 335, Train Loss: 2.3527, Val Loss: 2.3082\n",
      "Epoch: 336, Train Loss: 2.3695, Val Loss: 2.2595\n",
      "Epoch: 337, Train Loss: 2.3613, Val Loss: 2.2634\n",
      "Epoch: 338, Train Loss: 2.3623, Val Loss: 2.2768\n",
      "Epoch: 339, Train Loss: 2.3596, Val Loss: 2.2836\n",
      "Epoch: 340, Train Loss: 2.3681, Val Loss: 2.2846\n",
      "Epoch: 341, Train Loss: 2.3563, Val Loss: 2.2368\n",
      "Epoch: 342, Train Loss: 2.3720, Val Loss: 2.2472\n",
      "Epoch: 343, Train Loss: 2.3639, Val Loss: 2.2914\n",
      "Epoch: 344, Train Loss: 2.3640, Val Loss: 2.2718\n",
      "Epoch: 345, Train Loss: 2.3454, Val Loss: 2.2617\n",
      "Epoch: 346, Train Loss: 2.3521, Val Loss: 2.2729\n",
      "Epoch: 347, Train Loss: 2.3625, Val Loss: 2.2408\n",
      "Epoch: 348, Train Loss: 2.3635, Val Loss: 2.2660\n",
      "Epoch: 349, Train Loss: 2.3595, Val Loss: 2.2715\n",
      "Epoch: 350, Train Loss: 2.3633, Val Loss: 2.3100\n",
      "Epoch: 351, Train Loss: 2.3548, Val Loss: 2.2473\n",
      "Epoch: 352, Train Loss: 2.3776, Val Loss: 2.3039\n",
      "Epoch: 353, Train Loss: 2.3594, Val Loss: 2.2538\n",
      "Epoch: 354, Train Loss: 2.3615, Val Loss: 2.2810\n",
      "Epoch: 355, Train Loss: 2.3615, Val Loss: 2.2762\n",
      "Epoch: 356, Train Loss: 2.3667, Val Loss: 2.2673\n",
      "Epoch: 357, Train Loss: 2.3606, Val Loss: 2.2604\n",
      "Epoch: 358, Train Loss: 2.3505, Val Loss: 2.2351\n",
      "Epoch: 359, Train Loss: 2.3580, Val Loss: 2.2640\n",
      "Epoch: 360, Train Loss: 2.3612, Val Loss: 2.2736\n",
      "Epoch: 361, Train Loss: 2.3451, Val Loss: 2.2347\n",
      "Epoch: 362, Train Loss: 2.3457, Val Loss: 2.2545\n",
      "Epoch: 363, Train Loss: 2.3402, Val Loss: 2.2594\n",
      "Epoch: 364, Train Loss: 2.3360, Val Loss: 2.2280\n",
      "Epoch: 365, Train Loss: 2.3525, Val Loss: 2.2569\n",
      "Epoch: 366, Train Loss: 2.3329, Val Loss: 2.2411\n",
      "Epoch: 367, Train Loss: 2.3261, Val Loss: 2.2283\n",
      "Epoch: 368, Train Loss: 2.3476, Val Loss: 2.2495\n",
      "Epoch: 369, Train Loss: 2.3431, Val Loss: 2.2619\n",
      "Epoch: 370, Train Loss: 2.3503, Val Loss: 2.2685\n",
      "Epoch: 371, Train Loss: 2.3420, Val Loss: 2.2393\n",
      "Epoch: 372, Train Loss: 2.3527, Val Loss: 2.2555\n",
      "Epoch: 373, Train Loss: 2.3544, Val Loss: 2.2618\n",
      "Epoch: 374, Train Loss: 2.3464, Val Loss: 2.2450\n",
      "Epoch: 375, Train Loss: 2.3512, Val Loss: 2.2872\n",
      "Epoch: 376, Train Loss: 2.3317, Val Loss: 2.2503\n",
      "Epoch: 377, Train Loss: 2.3429, Val Loss: 2.2386\n",
      "Epoch: 378, Train Loss: 2.3473, Val Loss: 2.2164\n",
      "Epoch: 379, Train Loss: 2.3556, Val Loss: 2.2574\n",
      "Epoch: 380, Train Loss: 2.3572, Val Loss: 2.2320\n",
      "Epoch: 381, Train Loss: 2.3186, Val Loss: 2.2568\n",
      "Epoch: 382, Train Loss: 2.3354, Val Loss: 2.2331\n",
      "Epoch: 383, Train Loss: 2.3471, Val Loss: 2.2363\n",
      "Epoch: 384, Train Loss: 2.3444, Val Loss: 2.2601\n",
      "Epoch: 385, Train Loss: 2.3331, Val Loss: 2.2330\n",
      "Epoch: 386, Train Loss: 2.3325, Val Loss: 2.2408\n",
      "Epoch: 387, Train Loss: 2.3506, Val Loss: 2.2427\n",
      "Epoch: 388, Train Loss: 2.3559, Val Loss: 2.2727\n",
      "Epoch: 389, Train Loss: 2.3542, Val Loss: 2.2335\n",
      "Epoch: 390, Train Loss: 2.3449, Val Loss: 2.2536\n",
      "Epoch: 391, Train Loss: 2.3388, Val Loss: 2.2270\n",
      "Epoch: 392, Train Loss: 2.3286, Val Loss: 2.2448\n",
      "Epoch: 393, Train Loss: 2.3413, Val Loss: 2.2344\n",
      "Epoch: 394, Train Loss: 2.3399, Val Loss: 2.2207\n",
      "Epoch: 395, Train Loss: 2.3403, Val Loss: 2.2044\n",
      "Epoch: 396, Train Loss: 2.3495, Val Loss: 2.2788\n",
      "Epoch: 397, Train Loss: 2.3297, Val Loss: 2.2277\n",
      "Epoch: 398, Train Loss: 2.3256, Val Loss: 2.2436\n",
      "Epoch: 399, Train Loss: 2.3075, Val Loss: 2.2145\n",
      "Epoch: 400, Train Loss: 2.3201, Val Loss: 2.2459\n",
      "Epoch: 401, Train Loss: 2.3222, Val Loss: 2.2453\n",
      "Epoch: 402, Train Loss: 2.3287, Val Loss: 2.2425\n",
      "Epoch: 403, Train Loss: 2.3164, Val Loss: 2.2361\n",
      "Epoch: 404, Train Loss: 2.3388, Val Loss: 2.2329\n",
      "Epoch: 405, Train Loss: 2.3225, Val Loss: 2.2399\n",
      "Epoch: 406, Train Loss: 2.3356, Val Loss: 2.2250\n",
      "Epoch: 407, Train Loss: 2.3132, Val Loss: 2.2274\n",
      "Epoch: 408, Train Loss: 2.3224, Val Loss: 2.2215\n",
      "Epoch: 409, Train Loss: 2.3336, Val Loss: 2.2104\n",
      "Epoch: 410, Train Loss: 2.3148, Val Loss: 2.2162\n",
      "Epoch: 411, Train Loss: 2.3158, Val Loss: 2.1965\n",
      "Epoch: 412, Train Loss: 2.3280, Val Loss: 2.2350\n",
      "Epoch: 413, Train Loss: 2.3200, Val Loss: 2.2354\n",
      "Epoch: 414, Train Loss: 2.3208, Val Loss: 2.2538\n",
      "Epoch: 415, Train Loss: 2.3297, Val Loss: 2.2388\n",
      "Epoch: 416, Train Loss: 2.3187, Val Loss: 2.2165\n",
      "Epoch: 417, Train Loss: 2.3202, Val Loss: 2.2351\n",
      "Epoch: 418, Train Loss: 2.3181, Val Loss: 2.2271\n",
      "Epoch: 419, Train Loss: 2.3229, Val Loss: 2.2240\n",
      "Epoch: 420, Train Loss: 2.3213, Val Loss: 2.2178\n",
      "Epoch: 421, Train Loss: 2.3026, Val Loss: 2.2004\n",
      "Epoch: 422, Train Loss: 2.3019, Val Loss: 2.2240\n",
      "Epoch: 423, Train Loss: 2.3152, Val Loss: 2.2029\n",
      "Epoch: 424, Train Loss: 2.3308, Val Loss: 2.1975\n",
      "Epoch: 425, Train Loss: 2.3128, Val Loss: 2.2156\n",
      "Epoch: 426, Train Loss: 2.3282, Val Loss: 2.2378\n",
      "Epoch: 427, Train Loss: 2.3206, Val Loss: 2.2312\n",
      "Epoch: 428, Train Loss: 2.3101, Val Loss: 2.2022\n",
      "Epoch: 429, Train Loss: 2.3198, Val Loss: 2.2411\n",
      "Epoch: 430, Train Loss: 2.3128, Val Loss: 2.2311\n",
      "Epoch: 431, Train Loss: 2.3202, Val Loss: 2.2110\n",
      "Epoch: 432, Train Loss: 2.3149, Val Loss: 2.2075\n",
      "Epoch: 433, Train Loss: 2.3002, Val Loss: 2.2407\n",
      "Epoch: 434, Train Loss: 2.3045, Val Loss: 2.2141\n",
      "Epoch: 435, Train Loss: 2.2959, Val Loss: 2.2283\n",
      "Epoch: 436, Train Loss: 2.3049, Val Loss: 2.2110\n",
      "Epoch: 437, Train Loss: 2.3094, Val Loss: 2.2081\n",
      "Epoch: 438, Train Loss: 2.2945, Val Loss: 2.2009\n",
      "Epoch: 439, Train Loss: 2.3062, Val Loss: 2.1954\n",
      "Epoch: 440, Train Loss: 2.2931, Val Loss: 2.1689\n",
      "Epoch: 441, Train Loss: 2.3154, Val Loss: 2.2067\n",
      "Epoch: 442, Train Loss: 2.2981, Val Loss: 2.1738\n",
      "Epoch: 443, Train Loss: 2.2976, Val Loss: 2.2575\n",
      "Epoch: 444, Train Loss: 2.3030, Val Loss: 2.2149\n",
      "Epoch: 445, Train Loss: 2.2922, Val Loss: 2.1980\n",
      "Epoch: 446, Train Loss: 2.2943, Val Loss: 2.1949\n",
      "Epoch: 447, Train Loss: 2.2983, Val Loss: 2.1929\n",
      "Epoch: 448, Train Loss: 2.3000, Val Loss: 2.2133\n",
      "Epoch: 449, Train Loss: 2.3150, Val Loss: 2.1742\n",
      "Epoch: 450, Train Loss: 2.2893, Val Loss: 2.1767\n",
      "Epoch: 451, Train Loss: 2.3021, Val Loss: 2.1742\n",
      "Epoch: 452, Train Loss: 2.2831, Val Loss: 2.2164\n",
      "Epoch: 453, Train Loss: 2.2876, Val Loss: 2.1879\n",
      "Epoch: 454, Train Loss: 2.2669, Val Loss: 2.1813\n",
      "Epoch: 455, Train Loss: 2.2895, Val Loss: 2.1829\n",
      "Epoch: 456, Train Loss: 2.2737, Val Loss: 2.1923\n",
      "Epoch: 457, Train Loss: 2.2877, Val Loss: 2.1834\n",
      "Epoch: 458, Train Loss: 2.2933, Val Loss: 2.1902\n",
      "Epoch: 459, Train Loss: 2.2761, Val Loss: 2.2092\n",
      "Epoch: 460, Train Loss: 2.2728, Val Loss: 2.2165\n",
      "Epoch: 461, Train Loss: 2.2889, Val Loss: 2.1724\n",
      "Epoch: 462, Train Loss: 2.2973, Val Loss: 2.1821\n",
      "Epoch: 463, Train Loss: 2.2610, Val Loss: 2.2192\n",
      "Epoch: 464, Train Loss: 2.2661, Val Loss: 2.1693\n",
      "Epoch: 465, Train Loss: 2.2731, Val Loss: 2.1823\n",
      "Epoch: 466, Train Loss: 2.2888, Val Loss: 2.1704\n",
      "Epoch: 467, Train Loss: 2.2756, Val Loss: 2.1811\n",
      "Epoch: 468, Train Loss: 2.2883, Val Loss: 2.2117\n",
      "Epoch: 469, Train Loss: 2.2762, Val Loss: 2.1778\n",
      "Epoch: 470, Train Loss: 2.2823, Val Loss: 2.1699\n",
      "Epoch: 471, Train Loss: 2.2774, Val Loss: 2.2139\n",
      "Epoch: 472, Train Loss: 2.2744, Val Loss: 2.2037\n",
      "Epoch: 473, Train Loss: 2.2893, Val Loss: 2.1756\n",
      "Epoch: 474, Train Loss: 2.2767, Val Loss: 2.1853\n",
      "Epoch: 475, Train Loss: 2.2757, Val Loss: 2.1767\n",
      "Epoch: 476, Train Loss: 2.2849, Val Loss: 2.1899\n",
      "Epoch: 477, Train Loss: 2.2718, Val Loss: 2.1991\n",
      "Epoch: 478, Train Loss: 2.2741, Val Loss: 2.1912\n",
      "Epoch: 479, Train Loss: 2.2556, Val Loss: 2.1892\n",
      "Epoch: 480, Train Loss: 2.2712, Val Loss: 2.1823\n",
      "Epoch: 481, Train Loss: 2.2470, Val Loss: 2.1801\n",
      "Epoch: 482, Train Loss: 2.2668, Val Loss: 2.1698\n",
      "Epoch: 483, Train Loss: 2.2680, Val Loss: 2.2055\n",
      "Epoch: 484, Train Loss: 2.2632, Val Loss: 2.1713\n",
      "Epoch: 485, Train Loss: 2.2555, Val Loss: 2.1451\n",
      "Epoch: 486, Train Loss: 2.2607, Val Loss: 2.2214\n",
      "Epoch: 487, Train Loss: 2.2788, Val Loss: 2.2003\n",
      "Epoch: 488, Train Loss: 2.2766, Val Loss: 2.1799\n",
      "Epoch: 489, Train Loss: 2.2639, Val Loss: 2.1488\n",
      "Epoch: 490, Train Loss: 2.2644, Val Loss: 2.1665\n",
      "Epoch: 491, Train Loss: 2.2624, Val Loss: 2.1795\n",
      "Epoch: 492, Train Loss: 2.2569, Val Loss: 2.1858\n",
      "Epoch: 493, Train Loss: 2.2736, Val Loss: 2.1663\n",
      "Epoch: 494, Train Loss: 2.2480, Val Loss: 2.1635\n",
      "Epoch: 495, Train Loss: 2.2521, Val Loss: 2.1967\n",
      "Epoch: 496, Train Loss: 2.2679, Val Loss: 2.1872\n",
      "Epoch: 497, Train Loss: 2.2493, Val Loss: 2.1825\n",
      "Epoch: 498, Train Loss: 2.2524, Val Loss: 2.1909\n",
      "Epoch: 499, Train Loss: 2.2698, Val Loss: 2.1616\n",
      "Epoch: 500, Train Loss: 2.2669, Val Loss: 2.1949\n",
      "Epoch: 501, Train Loss: 2.2479, Val Loss: 2.1520\n",
      "Epoch: 502, Train Loss: 2.2697, Val Loss: 2.1738\n",
      "Epoch: 503, Train Loss: 2.2722, Val Loss: 2.1908\n",
      "Epoch: 504, Train Loss: 2.2648, Val Loss: 2.1539\n",
      "Epoch: 505, Train Loss: 2.2561, Val Loss: 2.1804\n",
      "Epoch: 506, Train Loss: 2.2534, Val Loss: 2.1744\n",
      "Epoch: 507, Train Loss: 2.2526, Val Loss: 2.1436\n",
      "Epoch: 508, Train Loss: 2.2494, Val Loss: 2.1752\n",
      "Epoch: 509, Train Loss: 2.2448, Val Loss: 2.1766\n",
      "Epoch: 510, Train Loss: 2.2612, Val Loss: 2.1401\n",
      "Epoch: 511, Train Loss: 2.2529, Val Loss: 2.1830\n",
      "Epoch: 512, Train Loss: 2.2524, Val Loss: 2.1918\n",
      "Epoch: 513, Train Loss: 2.2563, Val Loss: 2.1597\n",
      "Epoch: 514, Train Loss: 2.2503, Val Loss: 2.1541\n",
      "Epoch: 515, Train Loss: 2.2425, Val Loss: 2.1352\n",
      "Epoch: 516, Train Loss: 2.2453, Val Loss: 2.1290\n",
      "Epoch: 517, Train Loss: 2.2370, Val Loss: 2.1665\n",
      "Epoch: 518, Train Loss: 2.2331, Val Loss: 2.1736\n",
      "Epoch: 519, Train Loss: 2.2241, Val Loss: 2.1689\n",
      "Epoch: 520, Train Loss: 2.2275, Val Loss: 2.1588\n",
      "Epoch: 521, Train Loss: 2.2503, Val Loss: 2.1561\n",
      "Epoch: 522, Train Loss: 2.2534, Val Loss: 2.1771\n",
      "Epoch: 523, Train Loss: 2.2513, Val Loss: 2.1665\n",
      "Epoch: 524, Train Loss: 2.2439, Val Loss: 2.1374\n",
      "Epoch: 525, Train Loss: 2.2307, Val Loss: 2.1462\n",
      "Epoch: 526, Train Loss: 2.2297, Val Loss: 2.1578\n",
      "Epoch: 527, Train Loss: 2.2341, Val Loss: 2.1349\n",
      "Epoch: 528, Train Loss: 2.2341, Val Loss: 2.1747\n",
      "Epoch: 529, Train Loss: 2.2255, Val Loss: 2.1517\n",
      "Epoch: 530, Train Loss: 2.2454, Val Loss: 2.1494\n",
      "Epoch: 531, Train Loss: 2.2288, Val Loss: 2.1727\n",
      "Epoch: 532, Train Loss: 2.2293, Val Loss: 2.1496\n",
      "Epoch: 533, Train Loss: 2.2284, Val Loss: 2.1548\n",
      "Epoch: 534, Train Loss: 2.2339, Val Loss: 2.1455\n",
      "Epoch: 535, Train Loss: 2.2182, Val Loss: 2.2166\n",
      "Epoch: 536, Train Loss: 2.2394, Val Loss: 2.1513\n",
      "Epoch: 537, Train Loss: 2.2464, Val Loss: 2.1422\n",
      "Epoch: 538, Train Loss: 2.2270, Val Loss: 2.1459\n",
      "Epoch: 539, Train Loss: 2.2279, Val Loss: 2.1445\n",
      "Epoch: 540, Train Loss: 2.2170, Val Loss: 2.1233\n",
      "Epoch: 541, Train Loss: 2.2165, Val Loss: 2.1381\n",
      "Epoch: 542, Train Loss: 2.2331, Val Loss: 2.1505\n",
      "Epoch: 543, Train Loss: 2.2289, Val Loss: 2.1712\n",
      "Epoch: 544, Train Loss: 2.2318, Val Loss: 2.1600\n",
      "Epoch: 545, Train Loss: 2.2539, Val Loss: 2.1453\n",
      "Epoch: 546, Train Loss: 2.2167, Val Loss: 2.1454\n",
      "Epoch: 547, Train Loss: 2.2163, Val Loss: 2.1565\n",
      "Epoch: 548, Train Loss: 2.2210, Val Loss: 2.2062\n",
      "Epoch: 549, Train Loss: 2.2431, Val Loss: 2.1956\n",
      "Epoch: 550, Train Loss: 2.2230, Val Loss: 2.1492\n",
      "Epoch: 551, Train Loss: 2.2058, Val Loss: 2.1488\n",
      "Epoch: 552, Train Loss: 2.2265, Val Loss: 2.1170\n",
      "Epoch: 553, Train Loss: 2.2264, Val Loss: 2.1303\n",
      "Epoch: 554, Train Loss: 2.2225, Val Loss: 2.1607\n",
      "Epoch: 555, Train Loss: 2.2031, Val Loss: 2.1580\n",
      "Epoch: 556, Train Loss: 2.2279, Val Loss: 2.1375\n",
      "Epoch: 557, Train Loss: 2.2135, Val Loss: 2.1374\n",
      "Epoch: 558, Train Loss: 2.2133, Val Loss: 2.1768\n",
      "Epoch: 559, Train Loss: 2.2113, Val Loss: 2.1531\n",
      "Epoch: 560, Train Loss: 2.1994, Val Loss: 2.1410\n",
      "Epoch: 561, Train Loss: 2.2122, Val Loss: 2.0971\n",
      "Epoch: 562, Train Loss: 2.2163, Val Loss: 2.1768\n",
      "Epoch: 563, Train Loss: 2.2148, Val Loss: 2.1363\n",
      "Epoch: 564, Train Loss: 2.1943, Val Loss: 2.1292\n",
      "Epoch: 565, Train Loss: 2.1858, Val Loss: 2.1324\n",
      "Epoch: 566, Train Loss: 2.2179, Val Loss: 2.1535\n",
      "Epoch: 567, Train Loss: 2.2072, Val Loss: 2.1496\n",
      "Epoch: 568, Train Loss: 2.2059, Val Loss: 2.1641\n",
      "Epoch: 569, Train Loss: 2.1930, Val Loss: 2.1268\n",
      "Epoch: 570, Train Loss: 2.2038, Val Loss: 2.1003\n",
      "Epoch: 571, Train Loss: 2.2158, Val Loss: 2.1311\n",
      "Epoch: 572, Train Loss: 2.2117, Val Loss: 2.1474\n",
      "Epoch: 573, Train Loss: 2.2132, Val Loss: 2.1379\n",
      "Epoch: 574, Train Loss: 2.1951, Val Loss: 2.1438\n",
      "Epoch: 575, Train Loss: 2.2088, Val Loss: 2.1463\n",
      "Epoch: 576, Train Loss: 2.2133, Val Loss: 2.1411\n",
      "Epoch: 577, Train Loss: 2.1984, Val Loss: 2.1424\n",
      "Epoch: 578, Train Loss: 2.2049, Val Loss: 2.0982\n",
      "Epoch: 579, Train Loss: 2.1845, Val Loss: 2.1365\n",
      "Epoch: 580, Train Loss: 2.1992, Val Loss: 2.1216\n",
      "Epoch: 581, Train Loss: 2.2080, Val Loss: 2.1444\n",
      "Epoch: 582, Train Loss: 2.2043, Val Loss: 2.1608\n",
      "Epoch: 583, Train Loss: 2.2040, Val Loss: 2.1064\n",
      "Epoch: 584, Train Loss: 2.1871, Val Loss: 2.1450\n",
      "Epoch: 585, Train Loss: 2.1943, Val Loss: 2.1599\n",
      "Epoch: 586, Train Loss: 2.2061, Val Loss: 2.1257\n",
      "Epoch: 587, Train Loss: 2.1831, Val Loss: 2.1090\n",
      "Epoch: 588, Train Loss: 2.1763, Val Loss: 2.0957\n",
      "Epoch: 589, Train Loss: 2.2040, Val Loss: 2.1114\n",
      "Epoch: 590, Train Loss: 2.2010, Val Loss: 2.1189\n",
      "Epoch: 591, Train Loss: 2.1792, Val Loss: 2.1057\n",
      "Epoch: 592, Train Loss: 2.1919, Val Loss: 2.1247\n",
      "Epoch: 593, Train Loss: 2.1800, Val Loss: 2.1216\n",
      "Epoch: 594, Train Loss: 2.2008, Val Loss: 2.1521\n",
      "Epoch: 595, Train Loss: 2.2110, Val Loss: 2.0943\n",
      "Epoch: 596, Train Loss: 2.1776, Val Loss: 2.1199\n",
      "Epoch: 597, Train Loss: 2.1692, Val Loss: 2.0892\n",
      "Epoch: 598, Train Loss: 2.1935, Val Loss: 2.1344\n",
      "Epoch: 599, Train Loss: 2.1810, Val Loss: 2.1335\n",
      "Epoch: 600, Train Loss: 2.1940, Val Loss: 2.1315\n",
      "Epoch: 601, Train Loss: 2.1833, Val Loss: 2.1141\n",
      "Epoch: 602, Train Loss: 2.1900, Val Loss: 2.1079\n",
      "Epoch: 603, Train Loss: 2.2019, Val Loss: 2.0856\n",
      "Epoch: 604, Train Loss: 2.2018, Val Loss: 2.1523\n",
      "Epoch: 605, Train Loss: 2.1691, Val Loss: 2.0870\n",
      "Epoch: 606, Train Loss: 2.1731, Val Loss: 2.1208\n",
      "Epoch: 607, Train Loss: 2.1765, Val Loss: 2.1100\n",
      "Epoch: 608, Train Loss: 2.1919, Val Loss: 2.1313\n",
      "Epoch: 609, Train Loss: 2.1966, Val Loss: 2.0789\n",
      "Epoch: 610, Train Loss: 2.2000, Val Loss: 2.1078\n",
      "Epoch: 611, Train Loss: 2.1763, Val Loss: 2.1041\n",
      "Epoch: 612, Train Loss: 2.1666, Val Loss: 2.1413\n",
      "Epoch: 613, Train Loss: 2.1805, Val Loss: 2.1289\n",
      "Epoch: 614, Train Loss: 2.1927, Val Loss: 2.1048\n",
      "Epoch: 615, Train Loss: 2.1833, Val Loss: 2.1202\n",
      "Epoch: 616, Train Loss: 2.1542, Val Loss: 2.1328\n",
      "Epoch: 617, Train Loss: 2.1725, Val Loss: 2.1194\n",
      "Epoch: 618, Train Loss: 2.1679, Val Loss: 2.1503\n",
      "Epoch: 619, Train Loss: 2.1863, Val Loss: 2.1234\n",
      "Epoch: 620, Train Loss: 2.1759, Val Loss: 2.1075\n",
      "Epoch: 621, Train Loss: 2.1869, Val Loss: 2.0992\n",
      "Epoch: 622, Train Loss: 2.1555, Val Loss: 2.1094\n",
      "Epoch: 623, Train Loss: 2.1830, Val Loss: 2.1240\n",
      "Epoch: 624, Train Loss: 2.1828, Val Loss: 2.0952\n",
      "Epoch: 625, Train Loss: 2.1793, Val Loss: 2.0861\n",
      "Epoch: 626, Train Loss: 2.1707, Val Loss: 2.1255\n",
      "Epoch: 627, Train Loss: 2.1718, Val Loss: 2.0964\n",
      "Epoch: 628, Train Loss: 2.1734, Val Loss: 2.1015\n",
      "Epoch: 629, Train Loss: 2.1441, Val Loss: 2.0950\n",
      "Epoch: 630, Train Loss: 2.1669, Val Loss: 2.1258\n",
      "Epoch: 631, Train Loss: 2.1752, Val Loss: 2.1270\n",
      "Epoch: 632, Train Loss: 2.1657, Val Loss: 2.0828\n",
      "Epoch: 633, Train Loss: 2.1574, Val Loss: 2.1412\n",
      "Epoch: 634, Train Loss: 2.1486, Val Loss: 2.0755\n",
      "Epoch: 635, Train Loss: 2.1526, Val Loss: 2.1174\n",
      "Epoch: 636, Train Loss: 2.1399, Val Loss: 2.1358\n",
      "Epoch: 637, Train Loss: 2.1802, Val Loss: 2.1049\n",
      "Epoch: 638, Train Loss: 2.1614, Val Loss: 2.1303\n",
      "Epoch: 639, Train Loss: 2.1512, Val Loss: 2.0694\n",
      "Epoch: 640, Train Loss: 2.1526, Val Loss: 2.1200\n",
      "Epoch: 641, Train Loss: 2.1443, Val Loss: 2.1016\n",
      "Epoch: 642, Train Loss: 2.1596, Val Loss: 2.0838\n",
      "Epoch: 643, Train Loss: 2.1641, Val Loss: 2.1057\n",
      "Epoch: 644, Train Loss: 2.1500, Val Loss: 2.1155\n",
      "Epoch: 645, Train Loss: 2.1464, Val Loss: 2.0962\n",
      "Epoch: 646, Train Loss: 2.1687, Val Loss: 2.0864\n",
      "Epoch: 647, Train Loss: 2.1671, Val Loss: 2.1032\n",
      "Epoch: 648, Train Loss: 2.1451, Val Loss: 2.1123\n",
      "Epoch: 649, Train Loss: 2.1343, Val Loss: 2.1034\n",
      "Epoch: 650, Train Loss: 2.1590, Val Loss: 2.0916\n",
      "Epoch: 651, Train Loss: 2.1623, Val Loss: 2.1456\n",
      "Epoch: 652, Train Loss: 2.1516, Val Loss: 2.1275\n",
      "Epoch: 653, Train Loss: 2.1440, Val Loss: 2.1283\n",
      "Epoch: 654, Train Loss: 2.1471, Val Loss: 2.0996\n",
      "Epoch: 655, Train Loss: 2.1502, Val Loss: 2.0876\n",
      "Epoch: 656, Train Loss: 2.1503, Val Loss: 2.1091\n",
      "Epoch: 657, Train Loss: 2.1423, Val Loss: 2.0915\n",
      "Epoch: 658, Train Loss: 2.1480, Val Loss: 2.0794\n",
      "Epoch: 659, Train Loss: 2.1543, Val Loss: 2.1223\n",
      "Epoch: 660, Train Loss: 2.1683, Val Loss: 2.0618\n",
      "Epoch: 661, Train Loss: 2.1317, Val Loss: 2.0525\n",
      "Epoch: 662, Train Loss: 2.1339, Val Loss: 2.1026\n",
      "Epoch: 663, Train Loss: 2.1540, Val Loss: 2.1130\n",
      "Epoch: 664, Train Loss: 2.1269, Val Loss: 2.1035\n",
      "Epoch: 665, Train Loss: 2.1512, Val Loss: 2.0980\n",
      "Epoch: 666, Train Loss: 2.1494, Val Loss: 2.0924\n",
      "Epoch: 667, Train Loss: 2.1595, Val Loss: 2.0912\n",
      "Epoch: 668, Train Loss: 2.1644, Val Loss: 2.0842\n",
      "Epoch: 669, Train Loss: 2.1512, Val Loss: 2.1034\n",
      "Epoch: 670, Train Loss: 2.1438, Val Loss: 2.0770\n",
      "Epoch: 671, Train Loss: 2.1413, Val Loss: 2.0969\n",
      "Epoch: 672, Train Loss: 2.1603, Val Loss: 2.0755\n",
      "Epoch: 673, Train Loss: 2.1479, Val Loss: 2.0778\n",
      "Epoch: 674, Train Loss: 2.1282, Val Loss: 2.0771\n",
      "Epoch: 675, Train Loss: 2.1468, Val Loss: 2.1198\n",
      "Epoch: 676, Train Loss: 2.1577, Val Loss: 2.0770\n",
      "Epoch: 677, Train Loss: 2.1474, Val Loss: 2.0857\n",
      "Epoch: 678, Train Loss: 2.1377, Val Loss: 2.1169\n",
      "Epoch: 679, Train Loss: 2.1235, Val Loss: 2.0985\n",
      "Epoch: 680, Train Loss: 2.1341, Val Loss: 2.0964\n",
      "Epoch: 681, Train Loss: 2.1408, Val Loss: 2.1133\n",
      "Epoch: 682, Train Loss: 2.1464, Val Loss: 2.0988\n",
      "Epoch: 683, Train Loss: 2.1331, Val Loss: 2.0924\n",
      "Epoch: 684, Train Loss: 2.1275, Val Loss: 2.0620\n",
      "Epoch: 685, Train Loss: 2.1299, Val Loss: 2.0963\n",
      "Epoch: 686, Train Loss: 2.1489, Val Loss: 2.1179\n",
      "Epoch: 687, Train Loss: 2.1325, Val Loss: 2.1045\n",
      "Epoch: 688, Train Loss: 2.1278, Val Loss: 2.0862\n",
      "Epoch: 689, Train Loss: 2.1228, Val Loss: 2.0831\n",
      "Epoch: 690, Train Loss: 2.1344, Val Loss: 2.0672\n",
      "Epoch: 691, Train Loss: 2.1190, Val Loss: 2.0900\n",
      "Epoch: 692, Train Loss: 2.1233, Val Loss: 2.0809\n",
      "Epoch: 693, Train Loss: 2.1361, Val Loss: 2.1124\n",
      "Epoch: 694, Train Loss: 2.1354, Val Loss: 2.0785\n",
      "Epoch: 695, Train Loss: 2.1091, Val Loss: 2.0983\n",
      "Epoch: 696, Train Loss: 2.1323, Val Loss: 2.0617\n",
      "Epoch: 697, Train Loss: 2.1414, Val Loss: 2.0699\n",
      "Epoch: 698, Train Loss: 2.1075, Val Loss: 2.0915\n",
      "Epoch: 699, Train Loss: 2.1366, Val Loss: 2.1047\n",
      "Epoch: 700, Train Loss: 2.1247, Val Loss: 2.0898\n",
      "Epoch: 701, Train Loss: 2.1319, Val Loss: 2.0602\n",
      "Epoch: 702, Train Loss: 2.1443, Val Loss: 2.0786\n",
      "Epoch: 703, Train Loss: 2.1292, Val Loss: 2.0714\n",
      "Epoch: 704, Train Loss: 2.1261, Val Loss: 2.0689\n",
      "Epoch: 705, Train Loss: 2.1152, Val Loss: 2.0811\n",
      "Epoch: 706, Train Loss: 2.1225, Val Loss: 2.0970\n",
      "Epoch: 707, Train Loss: 2.1260, Val Loss: 2.1052\n",
      "Epoch: 708, Train Loss: 2.1306, Val Loss: 2.0818\n",
      "Epoch: 709, Train Loss: 2.1348, Val Loss: 2.0677\n",
      "Epoch: 710, Train Loss: 2.1211, Val Loss: 2.0887\n",
      "Epoch: 711, Train Loss: 2.1399, Val Loss: 2.1183\n",
      "Epoch: 712, Train Loss: 2.1225, Val Loss: 2.0470\n",
      "Epoch: 713, Train Loss: 2.1326, Val Loss: 2.0986\n",
      "Epoch: 714, Train Loss: 2.1090, Val Loss: 2.0881\n",
      "Epoch: 715, Train Loss: 2.1109, Val Loss: 2.0742\n",
      "Epoch: 716, Train Loss: 2.1151, Val Loss: 2.1237\n",
      "Epoch: 717, Train Loss: 2.1291, Val Loss: 2.0791\n",
      "Epoch: 718, Train Loss: 2.1210, Val Loss: 2.0524\n",
      "Epoch: 719, Train Loss: 2.0984, Val Loss: 2.0457\n",
      "Epoch: 720, Train Loss: 2.1174, Val Loss: 2.0609\n",
      "Epoch: 721, Train Loss: 2.1088, Val Loss: 2.0991\n",
      "Epoch: 722, Train Loss: 2.1197, Val Loss: 2.0414\n",
      "Epoch: 723, Train Loss: 2.1117, Val Loss: 2.0713\n",
      "Epoch: 724, Train Loss: 2.1285, Val Loss: 2.1291\n",
      "Epoch: 725, Train Loss: 2.1203, Val Loss: 2.0592\n",
      "Epoch: 726, Train Loss: 2.1226, Val Loss: 2.1007\n",
      "Epoch: 727, Train Loss: 2.1174, Val Loss: 2.1039\n",
      "Epoch: 728, Train Loss: 2.1147, Val Loss: 2.1037\n",
      "Epoch: 729, Train Loss: 2.1175, Val Loss: 2.0739\n",
      "Epoch: 730, Train Loss: 2.0989, Val Loss: 2.0576\n",
      "Epoch: 731, Train Loss: 2.1176, Val Loss: 2.0383\n",
      "Epoch: 732, Train Loss: 2.0986, Val Loss: 2.0782\n",
      "Epoch: 733, Train Loss: 2.1200, Val Loss: 2.1000\n",
      "Epoch: 734, Train Loss: 2.1063, Val Loss: 2.0759\n",
      "Epoch: 735, Train Loss: 2.0987, Val Loss: 2.0739\n",
      "Epoch: 736, Train Loss: 2.1214, Val Loss: 2.0677\n",
      "Epoch: 737, Train Loss: 2.1121, Val Loss: 2.1437\n",
      "Epoch: 738, Train Loss: 2.1091, Val Loss: 2.0822\n",
      "Epoch: 739, Train Loss: 2.0994, Val Loss: 2.0359\n",
      "Epoch: 740, Train Loss: 2.1131, Val Loss: 2.0804\n",
      "Epoch: 741, Train Loss: 2.0959, Val Loss: 2.0625\n",
      "Epoch: 742, Train Loss: 2.1211, Val Loss: 2.0975\n",
      "Epoch: 743, Train Loss: 2.1150, Val Loss: 2.0896\n",
      "Epoch: 744, Train Loss: 2.1191, Val Loss: 2.0641\n",
      "Epoch: 745, Train Loss: 2.1122, Val Loss: 2.0654\n",
      "Epoch: 746, Train Loss: 2.1148, Val Loss: 2.0807\n",
      "Epoch: 747, Train Loss: 2.1136, Val Loss: 2.0718\n",
      "Epoch: 748, Train Loss: 2.1035, Val Loss: 2.0860\n",
      "Epoch: 749, Train Loss: 2.0876, Val Loss: 2.0717\n",
      "Epoch: 750, Train Loss: 2.0953, Val Loss: 2.0804\n",
      "Epoch: 751, Train Loss: 2.1122, Val Loss: 2.0452\n",
      "Epoch: 752, Train Loss: 2.0969, Val Loss: 2.0952\n",
      "Epoch: 753, Train Loss: 2.0768, Val Loss: 2.0385\n",
      "Epoch: 754, Train Loss: 2.1040, Val Loss: 2.0666\n",
      "Epoch: 755, Train Loss: 2.1003, Val Loss: 2.0379\n",
      "Epoch: 756, Train Loss: 2.1017, Val Loss: 2.0910\n",
      "Epoch: 757, Train Loss: 2.0842, Val Loss: 2.0495\n",
      "Epoch: 758, Train Loss: 2.1044, Val Loss: 2.0707\n",
      "Epoch: 759, Train Loss: 2.0909, Val Loss: 2.0841\n",
      "Epoch: 760, Train Loss: 2.1109, Val Loss: 2.0850\n",
      "Epoch: 761, Train Loss: 2.0749, Val Loss: 2.0977\n",
      "Epoch: 762, Train Loss: 2.1136, Val Loss: 2.0747\n",
      "Epoch: 763, Train Loss: 2.0942, Val Loss: 2.0526\n",
      "Epoch: 764, Train Loss: 2.0922, Val Loss: 2.0124\n",
      "Epoch: 765, Train Loss: 2.0918, Val Loss: 2.0692\n",
      "Epoch: 766, Train Loss: 2.1286, Val Loss: 2.0447\n",
      "Epoch: 767, Train Loss: 2.0903, Val Loss: 2.1219\n",
      "Epoch: 768, Train Loss: 2.0724, Val Loss: 2.0436\n",
      "Epoch: 769, Train Loss: 2.1073, Val Loss: 2.0379\n",
      "Epoch: 770, Train Loss: 2.1023, Val Loss: 2.0931\n",
      "Epoch: 771, Train Loss: 2.0909, Val Loss: 2.0777\n",
      "Epoch: 772, Train Loss: 2.0970, Val Loss: 2.0463\n",
      "Epoch: 773, Train Loss: 2.1005, Val Loss: 2.0886\n",
      "Epoch: 774, Train Loss: 2.0809, Val Loss: 2.0749\n",
      "Epoch: 775, Train Loss: 2.0571, Val Loss: 2.0738\n",
      "Epoch: 776, Train Loss: 2.0984, Val Loss: 2.0299\n",
      "Epoch: 777, Train Loss: 2.1003, Val Loss: 2.0938\n",
      "Epoch: 778, Train Loss: 2.0962, Val Loss: 2.0542\n",
      "Epoch: 779, Train Loss: 2.0749, Val Loss: 2.0782\n",
      "Epoch: 780, Train Loss: 2.1047, Val Loss: 2.0738\n",
      "Epoch: 781, Train Loss: 2.0783, Val Loss: 2.0559\n",
      "Epoch: 782, Train Loss: 2.0864, Val Loss: 2.0947\n",
      "Epoch: 783, Train Loss: 2.0762, Val Loss: 2.0516\n",
      "Epoch: 784, Train Loss: 2.0864, Val Loss: 2.0948\n",
      "Epoch: 785, Train Loss: 2.0748, Val Loss: 2.0835\n",
      "Epoch: 786, Train Loss: 2.0677, Val Loss: 2.0572\n",
      "Epoch: 787, Train Loss: 2.0811, Val Loss: 2.0355\n",
      "Epoch: 788, Train Loss: 2.0874, Val Loss: 2.0359\n",
      "Epoch: 789, Train Loss: 2.0797, Val Loss: 2.0716\n",
      "Epoch: 790, Train Loss: 2.0964, Val Loss: 2.0498\n",
      "Epoch: 791, Train Loss: 2.1063, Val Loss: 2.0565\n",
      "Epoch: 792, Train Loss: 2.0988, Val Loss: 2.0685\n",
      "Epoch: 793, Train Loss: 2.0844, Val Loss: 2.0418\n",
      "Epoch: 794, Train Loss: 2.1031, Val Loss: 2.0279\n",
      "Epoch: 795, Train Loss: 2.0838, Val Loss: 2.0475\n",
      "Epoch: 796, Train Loss: 2.1030, Val Loss: 2.0451\n",
      "Epoch: 797, Train Loss: 2.1057, Val Loss: 2.0393\n",
      "Epoch: 798, Train Loss: 2.0986, Val Loss: 2.0431\n",
      "Epoch: 799, Train Loss: 2.0755, Val Loss: 2.0655\n",
      "Epoch: 800, Train Loss: 2.0756, Val Loss: 2.0499\n",
      "Epoch: 801, Train Loss: 2.0921, Val Loss: 2.0715\n",
      "Epoch: 802, Train Loss: 2.0719, Val Loss: 2.0866\n",
      "Epoch: 803, Train Loss: 2.0703, Val Loss: 2.0547\n",
      "Epoch: 804, Train Loss: 2.0831, Val Loss: 2.0390\n",
      "Epoch: 805, Train Loss: 2.0869, Val Loss: 2.0880\n",
      "Epoch: 806, Train Loss: 2.0595, Val Loss: 2.0835\n",
      "Epoch: 807, Train Loss: 2.0865, Val Loss: 2.0888\n",
      "Epoch: 808, Train Loss: 2.0895, Val Loss: 2.0545\n",
      "Epoch: 809, Train Loss: 2.0762, Val Loss: 2.0610\n",
      "Epoch: 810, Train Loss: 2.0842, Val Loss: 2.0432\n",
      "Epoch: 811, Train Loss: 2.0938, Val Loss: 2.0239\n",
      "Epoch: 812, Train Loss: 2.0671, Val Loss: 2.0678\n",
      "Epoch: 813, Train Loss: 2.0809, Val Loss: 2.1041\n",
      "Epoch: 814, Train Loss: 2.0830, Val Loss: 2.0838\n",
      "Epoch: 815, Train Loss: 2.0547, Val Loss: 2.0560\n",
      "Epoch: 816, Train Loss: 2.0760, Val Loss: 2.0263\n",
      "Epoch: 817, Train Loss: 2.0690, Val Loss: 2.0857\n",
      "Epoch: 818, Train Loss: 2.0885, Val Loss: 2.0377\n",
      "Epoch: 819, Train Loss: 2.0827, Val Loss: 2.0133\n",
      "Epoch: 820, Train Loss: 2.0555, Val Loss: 2.0233\n",
      "Epoch: 821, Train Loss: 2.0707, Val Loss: 2.0375\n",
      "Epoch: 822, Train Loss: 2.0688, Val Loss: 2.0321\n",
      "Epoch: 823, Train Loss: 2.0728, Val Loss: 2.0257\n",
      "Epoch: 824, Train Loss: 2.0817, Val Loss: 2.0493\n",
      "Epoch: 825, Train Loss: 2.0964, Val Loss: 2.0545\n",
      "Epoch: 826, Train Loss: 2.0716, Val Loss: 2.0701\n",
      "Epoch: 827, Train Loss: 2.0638, Val Loss: 2.0394\n",
      "Epoch: 828, Train Loss: 2.0503, Val Loss: 2.0661\n",
      "Epoch: 829, Train Loss: 2.0964, Val Loss: 2.0666\n",
      "Epoch: 830, Train Loss: 2.0736, Val Loss: 2.0614\n",
      "Epoch: 831, Train Loss: 2.0923, Val Loss: 2.0613\n",
      "Epoch: 832, Train Loss: 2.0792, Val Loss: 2.0555\n",
      "Epoch: 833, Train Loss: 2.0596, Val Loss: 2.0560\n",
      "Epoch: 834, Train Loss: 2.0925, Val Loss: 2.0684\n",
      "Epoch: 835, Train Loss: 2.0456, Val Loss: 2.0698\n",
      "Epoch: 836, Train Loss: 2.0718, Val Loss: 2.0584\n",
      "Epoch: 837, Train Loss: 2.0730, Val Loss: 2.0491\n",
      "Epoch: 838, Train Loss: 2.0831, Val Loss: 2.0595\n",
      "Epoch: 839, Train Loss: 2.0347, Val Loss: 2.0781\n",
      "Epoch: 840, Train Loss: 2.0685, Val Loss: 2.0357\n",
      "Epoch: 841, Train Loss: 2.0810, Val Loss: 2.0868\n",
      "Epoch: 842, Train Loss: 2.0662, Val Loss: 2.0326\n",
      "Epoch: 843, Train Loss: 2.0735, Val Loss: 2.0148\n",
      "Epoch: 844, Train Loss: 2.0415, Val Loss: 2.0445\n",
      "Epoch: 845, Train Loss: 2.0745, Val Loss: 2.0594\n",
      "Epoch: 846, Train Loss: 2.0821, Val Loss: 2.0416\n",
      "Epoch: 847, Train Loss: 2.0638, Val Loss: 2.0237\n",
      "Epoch: 848, Train Loss: 2.0656, Val Loss: 2.0446\n",
      "Epoch: 849, Train Loss: 2.0723, Val Loss: 2.0330\n",
      "Epoch: 850, Train Loss: 2.0528, Val Loss: 2.0567\n",
      "Epoch: 851, Train Loss: 2.0624, Val Loss: 2.0634\n",
      "Epoch: 852, Train Loss: 2.0794, Val Loss: 2.0274\n",
      "Epoch: 853, Train Loss: 2.0643, Val Loss: 2.0463\n",
      "Epoch: 854, Train Loss: 2.0404, Val Loss: 2.0140\n",
      "Epoch: 855, Train Loss: 2.0480, Val Loss: 2.0682\n",
      "Epoch: 856, Train Loss: 2.0516, Val Loss: 2.0052\n",
      "Epoch: 857, Train Loss: 2.0591, Val Loss: 2.0723\n",
      "Epoch: 858, Train Loss: 2.0704, Val Loss: 2.0392\n",
      "Epoch: 859, Train Loss: 2.0688, Val Loss: 2.0817\n",
      "Epoch: 860, Train Loss: 2.0658, Val Loss: 2.0869\n",
      "Epoch: 861, Train Loss: 2.0531, Val Loss: 2.0065\n",
      "Epoch: 862, Train Loss: 2.0516, Val Loss: 2.0491\n",
      "Epoch: 863, Train Loss: 2.0800, Val Loss: 2.0391\n",
      "Epoch: 864, Train Loss: 2.0503, Val Loss: 2.0758\n",
      "Epoch: 865, Train Loss: 2.0507, Val Loss: 2.0884\n",
      "Epoch: 866, Train Loss: 2.0344, Val Loss: 2.0652\n",
      "Epoch: 867, Train Loss: 2.0398, Val Loss: 2.0471\n",
      "Epoch: 868, Train Loss: 2.0472, Val Loss: 2.0495\n",
      "Epoch: 869, Train Loss: 2.0453, Val Loss: 2.0349\n",
      "Epoch: 870, Train Loss: 2.0365, Val Loss: 2.0488\n",
      "Epoch: 871, Train Loss: 2.0365, Val Loss: 2.0393\n",
      "Epoch: 872, Train Loss: 2.0554, Val Loss: 2.0357\n",
      "Epoch: 873, Train Loss: 2.0298, Val Loss: 2.0274\n",
      "Epoch: 874, Train Loss: 2.0745, Val Loss: 2.0277\n",
      "Epoch: 875, Train Loss: 2.0456, Val Loss: 2.0636\n",
      "Epoch: 876, Train Loss: 2.0471, Val Loss: 2.0140\n",
      "Epoch: 877, Train Loss: 2.0559, Val Loss: 2.0824\n",
      "Epoch: 878, Train Loss: 2.0572, Val Loss: 2.0410\n",
      "Epoch: 879, Train Loss: 2.0637, Val Loss: 2.0500\n",
      "Epoch: 880, Train Loss: 2.0493, Val Loss: 2.0164\n",
      "Epoch: 881, Train Loss: 2.0624, Val Loss: 2.0508\n",
      "Epoch: 882, Train Loss: 2.0513, Val Loss: 2.0462\n",
      "Epoch: 883, Train Loss: 2.0508, Val Loss: 2.0609\n",
      "Epoch: 884, Train Loss: 2.0692, Val Loss: 2.0498\n",
      "Epoch: 885, Train Loss: 2.0427, Val Loss: 2.0528\n",
      "Epoch: 886, Train Loss: 2.0522, Val Loss: 2.0449\n",
      "Epoch: 887, Train Loss: 2.0564, Val Loss: 2.0456\n",
      "Epoch: 888, Train Loss: 2.0764, Val Loss: 2.0355\n",
      "Epoch: 889, Train Loss: 2.0588, Val Loss: 2.0106\n",
      "Epoch: 890, Train Loss: 2.0481, Val Loss: 2.0767\n",
      "Epoch: 891, Train Loss: 2.0267, Val Loss: 2.0600\n",
      "Epoch: 892, Train Loss: 2.0606, Val Loss: 2.0500\n",
      "Epoch: 893, Train Loss: 2.0744, Val Loss: 2.0226\n",
      "Epoch: 894, Train Loss: 2.0523, Val Loss: 2.0187\n",
      "Epoch: 895, Train Loss: 2.0707, Val Loss: 2.0369\n",
      "Epoch: 896, Train Loss: 2.0416, Val Loss: 2.0270\n",
      "Epoch: 897, Train Loss: 2.0653, Val Loss: 2.0464\n",
      "Epoch: 898, Train Loss: 2.0528, Val Loss: 2.0473\n",
      "Epoch: 899, Train Loss: 2.0446, Val Loss: 2.0421\n",
      "Epoch: 900, Train Loss: 2.0473, Val Loss: 2.0170\n",
      "Epoch: 901, Train Loss: 2.0623, Val Loss: 2.0582\n",
      "Epoch: 902, Train Loss: 2.0350, Val Loss: 2.0650\n",
      "Epoch: 903, Train Loss: 2.0565, Val Loss: 2.0416\n",
      "Epoch: 904, Train Loss: 2.0453, Val Loss: 2.0332\n",
      "Epoch: 905, Train Loss: 2.0376, Val Loss: 2.0543\n",
      "Epoch: 906, Train Loss: 2.0427, Val Loss: 2.0141\n",
      "Epoch: 907, Train Loss: 2.0417, Val Loss: 2.0528\n",
      "Epoch: 908, Train Loss: 2.0317, Val Loss: 2.0324\n",
      "Epoch: 909, Train Loss: 2.0367, Val Loss: 2.0475\n",
      "Epoch: 910, Train Loss: 2.0409, Val Loss: 2.0134\n",
      "Epoch: 911, Train Loss: 2.0276, Val Loss: 2.0478\n",
      "Epoch: 912, Train Loss: 2.0383, Val Loss: 2.0833\n",
      "Epoch: 913, Train Loss: 2.0442, Val Loss: 2.0398\n",
      "Epoch: 914, Train Loss: 2.0466, Val Loss: 2.0048\n",
      "Epoch: 915, Train Loss: 2.0413, Val Loss: 2.0397\n",
      "Epoch: 916, Train Loss: 2.0452, Val Loss: 2.0430\n",
      "Epoch: 917, Train Loss: 2.0452, Val Loss: 2.0336\n",
      "Epoch: 918, Train Loss: 2.0096, Val Loss: 2.0799\n",
      "Epoch: 919, Train Loss: 2.0393, Val Loss: 2.0162\n",
      "Epoch: 920, Train Loss: 2.0242, Val Loss: 2.0392\n",
      "Epoch: 921, Train Loss: 2.0312, Val Loss: 2.0192\n",
      "Epoch: 922, Train Loss: 2.0530, Val Loss: 2.0415\n",
      "Epoch: 923, Train Loss: 2.0428, Val Loss: 2.0349\n",
      "Epoch: 924, Train Loss: 2.0521, Val Loss: 1.9928\n",
      "Epoch: 925, Train Loss: 2.0394, Val Loss: 2.0350\n",
      "Epoch: 926, Train Loss: 2.0443, Val Loss: 2.0143\n",
      "Epoch: 927, Train Loss: 2.0398, Val Loss: 2.0024\n",
      "Epoch: 928, Train Loss: 2.0455, Val Loss: 2.0064\n",
      "Epoch: 929, Train Loss: 2.0378, Val Loss: 2.0559\n",
      "Epoch: 930, Train Loss: 2.0404, Val Loss: 2.0285\n",
      "Epoch: 931, Train Loss: 2.0297, Val Loss: 2.0131\n",
      "Epoch: 932, Train Loss: 2.0593, Val Loss: 2.0268\n",
      "Epoch: 933, Train Loss: 2.0531, Val Loss: 2.0707\n",
      "Epoch: 934, Train Loss: 2.0374, Val Loss: 2.0160\n",
      "Epoch: 935, Train Loss: 2.0250, Val Loss: 2.0245\n",
      "Epoch: 936, Train Loss: 2.0396, Val Loss: 2.0259\n",
      "Epoch: 937, Train Loss: 2.0395, Val Loss: 1.9940\n",
      "Epoch: 938, Train Loss: 2.0362, Val Loss: 2.0243\n",
      "Epoch: 939, Train Loss: 2.0107, Val Loss: 2.0302\n",
      "Epoch: 940, Train Loss: 2.0341, Val Loss: 2.0172\n",
      "Epoch: 941, Train Loss: 2.0205, Val Loss: 2.0500\n",
      "Epoch: 942, Train Loss: 2.0518, Val Loss: 2.0302\n",
      "Epoch: 943, Train Loss: 2.0274, Val Loss: 2.0073\n",
      "Epoch: 944, Train Loss: 2.0504, Val Loss: 2.0167\n",
      "Epoch: 945, Train Loss: 2.0550, Val Loss: 2.0287\n",
      "Epoch: 946, Train Loss: 2.0401, Val Loss: 2.0213\n",
      "Epoch: 947, Train Loss: 2.0513, Val Loss: 2.0291\n",
      "Epoch: 948, Train Loss: 2.0398, Val Loss: 2.0203\n",
      "Epoch: 949, Train Loss: 2.0331, Val Loss: 1.9941\n",
      "Epoch: 950, Train Loss: 2.0430, Val Loss: 2.0233\n",
      "Epoch: 951, Train Loss: 2.0267, Val Loss: 1.9966\n",
      "Epoch: 952, Train Loss: 2.0359, Val Loss: 2.0293\n",
      "Epoch: 953, Train Loss: 2.0397, Val Loss: 2.0077\n",
      "Epoch: 954, Train Loss: 2.0284, Val Loss: 2.0038\n",
      "Epoch: 955, Train Loss: 2.0346, Val Loss: 1.9950\n",
      "Epoch: 956, Train Loss: 2.0368, Val Loss: 1.9865\n",
      "Epoch: 957, Train Loss: 2.0114, Val Loss: 2.0334\n",
      "Epoch: 958, Train Loss: 2.0406, Val Loss: 2.0248\n",
      "Epoch: 959, Train Loss: 2.0272, Val Loss: 1.9915\n",
      "Epoch: 960, Train Loss: 2.0147, Val Loss: 2.0599\n",
      "Epoch: 961, Train Loss: 2.0334, Val Loss: 2.0074\n",
      "Epoch: 962, Train Loss: 2.0272, Val Loss: 2.0270\n",
      "Epoch: 963, Train Loss: 2.0315, Val Loss: 2.0150\n",
      "Epoch: 964, Train Loss: 2.0162, Val Loss: 2.0475\n",
      "Epoch: 965, Train Loss: 2.0506, Val Loss: 2.0163\n",
      "Epoch: 966, Train Loss: 2.0256, Val Loss: 2.0421\n",
      "Epoch: 967, Train Loss: 2.0524, Val Loss: 2.0182\n",
      "Epoch: 968, Train Loss: 2.0069, Val Loss: 2.0106\n",
      "Epoch: 969, Train Loss: 2.0436, Val Loss: 2.0384\n",
      "Epoch: 970, Train Loss: 2.0322, Val Loss: 2.0125\n",
      "Epoch: 971, Train Loss: 2.0167, Val Loss: 2.0125\n",
      "Epoch: 972, Train Loss: 2.0354, Val Loss: 2.0589\n",
      "Epoch: 973, Train Loss: 2.0162, Val Loss: 2.0159\n",
      "Epoch: 974, Train Loss: 2.0325, Val Loss: 1.9963\n",
      "Epoch: 975, Train Loss: 2.0406, Val Loss: 2.0154\n",
      "Epoch: 976, Train Loss: 2.0236, Val Loss: 2.0334\n",
      "Epoch: 977, Train Loss: 2.0155, Val Loss: 2.0558\n",
      "Epoch: 978, Train Loss: 1.9924, Val Loss: 1.9985\n",
      "Epoch: 979, Train Loss: 2.0343, Val Loss: 2.0243\n",
      "Epoch: 980, Train Loss: 2.0293, Val Loss: 2.0136\n",
      "Epoch: 981, Train Loss: 2.0304, Val Loss: 2.0207\n",
      "Epoch: 982, Train Loss: 2.0168, Val Loss: 2.0216\n",
      "Epoch: 983, Train Loss: 2.0197, Val Loss: 1.9329\n",
      "Epoch: 984, Train Loss: 2.0181, Val Loss: 2.0016\n",
      "Epoch: 985, Train Loss: 2.0141, Val Loss: 2.0641\n",
      "Epoch: 986, Train Loss: 2.0351, Val Loss: 2.0003\n",
      "Epoch: 987, Train Loss: 2.0388, Val Loss: 1.9836\n",
      "Epoch: 988, Train Loss: 2.0033, Val Loss: 2.0091\n",
      "Epoch: 989, Train Loss: 2.0260, Val Loss: 2.0458\n",
      "Epoch: 990, Train Loss: 2.0319, Val Loss: 2.0496\n",
      "Epoch: 991, Train Loss: 2.0628, Val Loss: 2.0108\n",
      "Epoch: 992, Train Loss: 2.0164, Val Loss: 2.0042\n",
      "Epoch: 993, Train Loss: 1.9985, Val Loss: 2.0427\n",
      "Epoch: 994, Train Loss: 2.0211, Val Loss: 1.9745\n",
      "Epoch: 995, Train Loss: 2.0090, Val Loss: 2.0198\n",
      "Epoch: 996, Train Loss: 2.0307, Val Loss: 2.0232\n",
      "Epoch: 997, Train Loss: 1.9988, Val Loss: 1.9965\n",
      "Epoch: 998, Train Loss: 2.0284, Val Loss: 2.0117\n",
      "Epoch: 999, Train Loss: 2.0168, Val Loss: 1.9921\n",
      "Epoch: 1000, Train Loss: 2.0088, Val Loss: 2.0463\n",
      "Epoch: 1001, Train Loss: 2.0049, Val Loss: 2.0010\n",
      "Epoch: 1002, Train Loss: 1.9972, Val Loss: 2.0038\n",
      "Epoch: 1003, Train Loss: 2.0165, Val Loss: 1.9918\n",
      "Epoch: 1004, Train Loss: 2.0051, Val Loss: 2.0191\n",
      "Epoch: 1005, Train Loss: 2.0243, Val Loss: 1.9814\n",
      "Epoch: 1006, Train Loss: 2.0021, Val Loss: 2.0577\n",
      "Epoch: 1007, Train Loss: 2.0198, Val Loss: 2.0408\n",
      "Epoch: 1008, Train Loss: 2.0044, Val Loss: 1.9982\n",
      "Epoch: 1009, Train Loss: 2.0172, Val Loss: 2.0593\n",
      "Epoch: 1010, Train Loss: 1.9868, Val Loss: 2.0148\n",
      "Epoch: 1011, Train Loss: 2.0004, Val Loss: 2.0170\n",
      "Epoch: 1012, Train Loss: 2.0229, Val Loss: 1.9944\n",
      "Epoch: 1013, Train Loss: 1.9925, Val Loss: 2.0181\n",
      "Epoch: 1014, Train Loss: 1.9868, Val Loss: 2.0536\n",
      "Epoch: 1015, Train Loss: 1.9997, Val Loss: 2.0066\n",
      "Epoch: 1016, Train Loss: 2.0193, Val Loss: 2.0156\n",
      "Epoch: 1017, Train Loss: 2.0325, Val Loss: 1.9999\n",
      "Epoch: 1018, Train Loss: 2.0229, Val Loss: 2.0273\n",
      "Epoch: 1019, Train Loss: 2.0050, Val Loss: 2.0272\n",
      "Epoch: 1020, Train Loss: 2.0326, Val Loss: 1.9910\n",
      "Epoch: 1021, Train Loss: 2.0314, Val Loss: 2.0001\n",
      "Epoch: 1022, Train Loss: 2.0023, Val Loss: 2.0087\n",
      "Epoch: 1023, Train Loss: 2.0285, Val Loss: 2.0102\n",
      "Epoch: 1024, Train Loss: 2.0052, Val Loss: 1.9941\n",
      "Epoch: 1025, Train Loss: 2.0114, Val Loss: 1.9973\n",
      "Epoch: 1026, Train Loss: 2.0138, Val Loss: 2.0107\n",
      "Epoch: 1027, Train Loss: 2.0001, Val Loss: 2.0105\n",
      "Epoch: 1028, Train Loss: 2.0269, Val Loss: 1.9904\n",
      "Epoch: 1029, Train Loss: 2.0021, Val Loss: 2.0092\n",
      "Epoch: 1030, Train Loss: 2.0061, Val Loss: 2.0444\n",
      "Epoch: 1031, Train Loss: 1.9874, Val Loss: 2.0251\n",
      "Epoch: 1032, Train Loss: 1.9952, Val Loss: 2.0279\n",
      "Epoch: 1033, Train Loss: 2.0061, Val Loss: 1.9753\n",
      "Epoch: 1034, Train Loss: 2.0073, Val Loss: 1.9837\n",
      "Epoch: 1035, Train Loss: 1.9902, Val Loss: 1.9945\n",
      "Epoch: 1036, Train Loss: 2.0039, Val Loss: 1.9834\n",
      "Epoch: 1037, Train Loss: 2.0038, Val Loss: 1.9530\n",
      "Epoch: 1038, Train Loss: 2.0099, Val Loss: 1.9889\n",
      "Epoch: 1039, Train Loss: 1.9865, Val Loss: 1.9879\n",
      "Epoch: 1040, Train Loss: 2.0266, Val Loss: 2.0040\n",
      "Epoch: 1041, Train Loss: 2.0211, Val Loss: 2.0253\n",
      "Epoch: 1042, Train Loss: 2.0003, Val Loss: 2.0250\n",
      "Epoch: 1043, Train Loss: 1.9849, Val Loss: 1.9733\n",
      "Epoch: 1044, Train Loss: 1.9840, Val Loss: 1.9982\n",
      "Epoch: 1045, Train Loss: 2.0139, Val Loss: 1.9809\n",
      "Epoch: 1046, Train Loss: 2.0040, Val Loss: 2.0474\n",
      "Epoch: 1047, Train Loss: 2.0132, Val Loss: 2.0067\n",
      "Epoch: 1048, Train Loss: 2.0157, Val Loss: 1.9757\n",
      "Epoch: 1049, Train Loss: 2.0303, Val Loss: 2.0033\n",
      "Epoch: 1050, Train Loss: 2.0054, Val Loss: 1.9739\n",
      "Epoch: 1051, Train Loss: 1.9589, Val Loss: 2.0353\n",
      "Epoch: 1052, Train Loss: 2.0089, Val Loss: 1.9852\n",
      "Epoch: 1053, Train Loss: 1.9930, Val Loss: 2.0350\n",
      "Epoch: 1054, Train Loss: 2.0105, Val Loss: 1.9976\n",
      "Epoch: 1055, Train Loss: 2.0107, Val Loss: 2.0062\n",
      "Epoch: 1056, Train Loss: 2.0169, Val Loss: 2.0009\n",
      "Epoch: 1057, Train Loss: 2.0093, Val Loss: 2.0021\n",
      "Epoch: 1058, Train Loss: 1.9792, Val Loss: 1.9977\n",
      "Epoch: 1059, Train Loss: 2.0139, Val Loss: 1.9627\n",
      "Epoch: 1060, Train Loss: 1.9693, Val Loss: 1.9940\n",
      "Epoch: 1061, Train Loss: 2.0038, Val Loss: 2.0082\n",
      "Epoch: 1062, Train Loss: 1.9757, Val Loss: 1.9878\n",
      "Epoch: 1063, Train Loss: 1.9839, Val Loss: 1.9980\n",
      "Epoch: 1064, Train Loss: 1.9750, Val Loss: 1.9869\n",
      "Epoch: 1065, Train Loss: 1.9934, Val Loss: 1.9848\n",
      "Epoch: 1066, Train Loss: 2.0282, Val Loss: 2.0101\n",
      "Epoch: 1067, Train Loss: 1.9973, Val Loss: 1.9521\n",
      "Epoch: 1068, Train Loss: 1.9843, Val Loss: 1.9712\n",
      "Epoch: 1069, Train Loss: 1.9927, Val Loss: 1.9856\n",
      "Epoch: 1070, Train Loss: 1.9915, Val Loss: 2.0005\n",
      "Epoch: 1071, Train Loss: 1.9811, Val Loss: 1.9814\n",
      "Epoch: 1072, Train Loss: 2.0050, Val Loss: 1.9763\n",
      "Epoch: 1073, Train Loss: 1.9693, Val Loss: 1.9873\n",
      "Epoch: 1074, Train Loss: 1.9902, Val Loss: 1.9609\n",
      "Epoch: 1075, Train Loss: 2.0118, Val Loss: 2.0044\n",
      "Epoch: 1076, Train Loss: 1.9987, Val Loss: 1.9827\n",
      "Epoch: 1077, Train Loss: 2.0167, Val Loss: 1.9746\n",
      "Epoch: 1078, Train Loss: 1.9834, Val Loss: 1.9691\n",
      "Epoch: 1079, Train Loss: 1.9832, Val Loss: 1.9926\n",
      "Epoch: 1080, Train Loss: 1.9960, Val Loss: 2.0105\n",
      "Epoch: 1081, Train Loss: 1.9803, Val Loss: 1.9734\n",
      "Epoch: 1082, Train Loss: 1.9968, Val Loss: 2.0332\n",
      "Epoch: 1083, Train Loss: 2.0039, Val Loss: 2.0143\n",
      "Epoch: 1084, Train Loss: 1.9929, Val Loss: 1.9916\n",
      "Epoch: 1085, Train Loss: 1.9826, Val Loss: 2.0055\n",
      "Epoch: 1086, Train Loss: 2.0037, Val Loss: 1.9892\n",
      "Epoch: 1087, Train Loss: 1.9699, Val Loss: 2.0118\n",
      "Epoch: 1088, Train Loss: 1.9919, Val Loss: 2.0222\n",
      "Epoch: 1089, Train Loss: 1.9992, Val Loss: 1.9674\n",
      "Epoch: 1090, Train Loss: 1.9937, Val Loss: 1.9994\n",
      "Epoch: 1091, Train Loss: 1.9879, Val Loss: 1.9844\n",
      "Epoch: 1092, Train Loss: 1.9829, Val Loss: 1.9875\n",
      "Epoch: 1093, Train Loss: 1.9932, Val Loss: 1.9878\n",
      "Epoch: 1094, Train Loss: 1.9960, Val Loss: 1.9608\n",
      "Epoch: 1095, Train Loss: 1.9747, Val Loss: 1.9725\n",
      "Epoch: 1096, Train Loss: 2.0021, Val Loss: 1.9807\n",
      "Epoch: 1097, Train Loss: 2.0142, Val Loss: 1.9940\n",
      "Epoch: 1098, Train Loss: 1.9914, Val Loss: 1.9890\n",
      "Epoch: 1099, Train Loss: 1.9711, Val Loss: 2.0143\n",
      "Epoch: 1100, Train Loss: 1.9694, Val Loss: 2.0070\n",
      "Epoch: 1101, Train Loss: 1.9948, Val Loss: 2.0330\n",
      "Epoch: 1102, Train Loss: 2.0032, Val Loss: 1.9993\n",
      "Epoch: 1103, Train Loss: 1.9771, Val Loss: 2.0197\n",
      "Epoch: 1104, Train Loss: 2.0123, Val Loss: 1.9689\n",
      "Epoch: 1105, Train Loss: 2.0143, Val Loss: 1.9810\n",
      "Epoch: 1106, Train Loss: 1.9935, Val Loss: 2.0067\n",
      "Epoch: 1107, Train Loss: 1.9826, Val Loss: 1.9704\n",
      "Epoch: 1108, Train Loss: 1.9516, Val Loss: 1.9715\n",
      "Epoch: 1109, Train Loss: 1.9918, Val Loss: 1.9787\n",
      "Epoch: 1110, Train Loss: 2.0127, Val Loss: 2.0408\n",
      "Epoch: 1111, Train Loss: 1.9850, Val Loss: 2.0151\n",
      "Epoch: 1112, Train Loss: 1.9925, Val Loss: 2.0120\n",
      "Epoch: 1113, Train Loss: 1.9886, Val Loss: 1.9810\n",
      "Epoch: 1114, Train Loss: 1.9881, Val Loss: 2.0310\n",
      "Epoch: 1115, Train Loss: 1.9834, Val Loss: 1.9767\n",
      "Epoch: 1116, Train Loss: 1.9886, Val Loss: 1.9518\n",
      "Epoch: 1117, Train Loss: 1.9957, Val Loss: 1.9980\n",
      "Epoch: 1118, Train Loss: 1.9937, Val Loss: 1.9927\n",
      "Epoch: 1119, Train Loss: 1.9942, Val Loss: 1.9805\n",
      "Epoch: 1120, Train Loss: 1.9781, Val Loss: 2.0209\n",
      "Epoch: 1121, Train Loss: 1.9889, Val Loss: 1.9461\n",
      "Epoch: 1122, Train Loss: 1.9688, Val Loss: 2.0053\n",
      "Epoch: 1123, Train Loss: 1.9748, Val Loss: 1.9918\n",
      "Epoch: 1124, Train Loss: 1.9838, Val Loss: 1.9984\n",
      "Epoch: 1125, Train Loss: 1.9609, Val Loss: 1.9612\n",
      "Epoch: 1126, Train Loss: 1.9889, Val Loss: 1.9673\n",
      "Epoch: 1127, Train Loss: 1.9951, Val Loss: 1.9597\n",
      "Epoch: 1128, Train Loss: 1.9802, Val Loss: 1.9874\n",
      "Epoch: 1129, Train Loss: 1.9982, Val Loss: 2.0080\n",
      "Epoch: 1130, Train Loss: 1.9614, Val Loss: 1.9636\n",
      "Epoch: 1131, Train Loss: 1.9769, Val Loss: 1.9349\n",
      "Epoch: 1132, Train Loss: 1.9765, Val Loss: 1.9946\n",
      "Epoch: 1133, Train Loss: 1.9940, Val Loss: 1.9963\n",
      "Epoch: 1134, Train Loss: 1.9945, Val Loss: 1.9655\n",
      "Epoch: 1135, Train Loss: 1.9821, Val Loss: 2.0051\n",
      "Epoch: 1136, Train Loss: 1.9710, Val Loss: 1.9954\n",
      "Epoch: 1137, Train Loss: 1.9867, Val Loss: 1.9663\n",
      "Epoch: 1138, Train Loss: 1.9895, Val Loss: 1.9656\n",
      "Epoch: 1139, Train Loss: 1.9550, Val Loss: 1.9775\n",
      "Epoch: 1140, Train Loss: 1.9846, Val Loss: 2.0220\n",
      "Epoch: 1141, Train Loss: 1.9668, Val Loss: 1.9815\n",
      "Epoch: 1142, Train Loss: 2.0125, Val Loss: 2.0333\n",
      "Epoch: 1143, Train Loss: 1.9784, Val Loss: 1.9461\n",
      "Epoch: 1144, Train Loss: 1.9686, Val Loss: 1.9750\n",
      "Epoch: 1145, Train Loss: 2.0053, Val Loss: 1.9778\n",
      "Epoch: 1146, Train Loss: 1.9731, Val Loss: 1.9762\n",
      "Epoch: 1147, Train Loss: 1.9944, Val Loss: 1.9806\n",
      "Epoch: 1148, Train Loss: 2.0122, Val Loss: 1.9734\n",
      "Epoch: 1149, Train Loss: 1.9920, Val Loss: 1.9638\n",
      "Epoch: 1150, Train Loss: 1.9777, Val Loss: 1.9840\n",
      "Epoch: 1151, Train Loss: 1.9813, Val Loss: 1.9790\n",
      "Epoch: 1152, Train Loss: 2.0010, Val Loss: 1.9921\n",
      "Epoch: 1153, Train Loss: 1.9744, Val Loss: 1.9998\n",
      "Epoch: 1154, Train Loss: 1.9808, Val Loss: 1.9734\n",
      "Epoch: 1155, Train Loss: 1.9699, Val Loss: 1.9551\n",
      "Epoch: 1156, Train Loss: 1.9575, Val Loss: 2.0117\n",
      "Epoch: 1157, Train Loss: 1.9788, Val Loss: 1.9781\n",
      "Epoch: 1158, Train Loss: 1.9612, Val Loss: 1.9759\n",
      "Epoch: 1159, Train Loss: 1.9828, Val Loss: 1.9688\n",
      "Epoch: 1160, Train Loss: 1.9363, Val Loss: 1.9976\n",
      "Epoch: 1161, Train Loss: 1.9785, Val Loss: 2.0152\n",
      "Epoch: 1162, Train Loss: 1.9839, Val Loss: 1.9778\n",
      "Epoch: 1163, Train Loss: 1.9574, Val Loss: 1.9682\n",
      "Epoch: 1164, Train Loss: 1.9801, Val Loss: 1.9599\n",
      "Epoch: 1165, Train Loss: 1.9775, Val Loss: 1.9881\n",
      "Epoch: 1166, Train Loss: 1.9667, Val Loss: 1.9579\n",
      "Epoch: 1167, Train Loss: 1.9713, Val Loss: 1.9958\n",
      "Epoch: 1168, Train Loss: 1.9672, Val Loss: 2.0172\n",
      "Epoch: 1169, Train Loss: 1.9633, Val Loss: 1.9817\n",
      "Epoch: 1170, Train Loss: 2.0053, Val Loss: 2.0099\n",
      "Epoch: 1171, Train Loss: 1.9736, Val Loss: 1.9749\n",
      "Epoch: 1172, Train Loss: 1.9627, Val Loss: 1.9665\n",
      "Epoch: 1173, Train Loss: 1.9793, Val Loss: 1.9328\n",
      "Epoch: 1174, Train Loss: 1.9746, Val Loss: 1.9261\n",
      "Epoch: 1175, Train Loss: 1.9648, Val Loss: 1.9449\n",
      "Epoch: 1176, Train Loss: 1.9786, Val Loss: 1.9974\n",
      "Epoch: 1177, Train Loss: 1.9880, Val Loss: 1.9944\n",
      "Epoch: 1178, Train Loss: 1.9653, Val Loss: 1.9836\n",
      "Epoch: 1179, Train Loss: 1.9564, Val Loss: 1.9902\n",
      "Epoch: 1180, Train Loss: 1.9964, Val Loss: 1.9344\n",
      "Epoch: 1181, Train Loss: 1.9427, Val Loss: 1.9980\n",
      "Epoch: 1182, Train Loss: 1.9556, Val Loss: 1.9600\n",
      "Epoch: 1183, Train Loss: 1.9771, Val Loss: 1.9685\n",
      "Epoch: 1184, Train Loss: 1.9457, Val Loss: 2.0041\n",
      "Epoch: 1185, Train Loss: 1.9516, Val Loss: 1.9913\n",
      "Epoch: 1186, Train Loss: 1.9661, Val Loss: 1.9906\n",
      "Epoch: 1187, Train Loss: 1.9807, Val Loss: 1.9929\n",
      "Epoch: 1188, Train Loss: 1.9806, Val Loss: 1.9792\n",
      "Epoch: 1189, Train Loss: 1.9531, Val Loss: 1.9819\n",
      "Epoch: 1190, Train Loss: 1.9842, Val Loss: 1.9590\n",
      "Epoch: 1191, Train Loss: 1.9924, Val Loss: 1.9880\n",
      "Epoch: 1192, Train Loss: 1.9964, Val Loss: 1.9713\n",
      "Epoch: 1193, Train Loss: 1.9917, Val Loss: 1.9939\n",
      "Epoch: 1194, Train Loss: 1.9429, Val Loss: 1.9988\n",
      "Epoch: 1195, Train Loss: 1.9664, Val Loss: 1.9677\n",
      "Epoch: 1196, Train Loss: 1.9562, Val Loss: 1.9820\n",
      "Epoch: 1197, Train Loss: 1.9633, Val Loss: 2.0380\n",
      "Epoch: 1198, Train Loss: 1.9752, Val Loss: 2.0165\n",
      "Epoch: 1199, Train Loss: 1.9755, Val Loss: 1.9615\n",
      "Epoch: 1200, Train Loss: 1.9713, Val Loss: 1.9994\n",
      "Epoch: 1201, Train Loss: 1.9780, Val Loss: 1.9432\n",
      "Epoch: 1202, Train Loss: 1.9832, Val Loss: 1.9923\n",
      "Epoch: 1203, Train Loss: 1.9566, Val Loss: 1.9777\n",
      "Epoch: 1204, Train Loss: 1.9536, Val Loss: 1.9708\n",
      "Epoch: 1205, Train Loss: 1.9742, Val Loss: 1.9646\n",
      "Epoch: 1206, Train Loss: 1.9559, Val Loss: 1.9789\n",
      "Epoch: 1207, Train Loss: 1.9590, Val Loss: 2.0049\n",
      "Epoch: 1208, Train Loss: 1.9785, Val Loss: 1.9923\n",
      "Epoch: 1209, Train Loss: 1.9544, Val Loss: 1.9851\n",
      "Epoch: 1210, Train Loss: 1.9409, Val Loss: 1.9623\n",
      "Epoch: 1211, Train Loss: 1.9643, Val Loss: 1.9668\n",
      "Epoch: 1212, Train Loss: 1.9583, Val Loss: 1.9237\n",
      "Epoch: 1213, Train Loss: 1.9777, Val Loss: 1.9811\n",
      "Epoch: 1214, Train Loss: 1.9525, Val Loss: 1.9911\n",
      "Epoch: 1215, Train Loss: 1.9662, Val Loss: 1.9642\n",
      "Epoch: 1216, Train Loss: 1.9745, Val Loss: 1.9767\n",
      "Epoch: 1217, Train Loss: 1.9524, Val Loss: 1.9829\n",
      "Epoch: 1218, Train Loss: 1.9580, Val Loss: 1.9744\n",
      "Epoch: 1219, Train Loss: 1.9634, Val Loss: 1.9985\n",
      "Epoch: 1220, Train Loss: 1.9631, Val Loss: 1.9608\n",
      "Epoch: 1221, Train Loss: 1.9506, Val Loss: 1.9741\n",
      "Epoch: 1222, Train Loss: 1.9701, Val Loss: 2.0131\n",
      "Epoch: 1223, Train Loss: 1.9548, Val Loss: 1.9707\n",
      "Epoch: 1224, Train Loss: 1.9702, Val Loss: 1.9555\n",
      "Epoch: 1225, Train Loss: 1.9823, Val Loss: 1.9656\n",
      "Epoch: 1226, Train Loss: 1.9575, Val Loss: 2.0061\n",
      "Epoch: 1227, Train Loss: 1.9494, Val Loss: 2.0294\n",
      "Epoch: 1228, Train Loss: 1.9629, Val Loss: 1.9757\n",
      "Epoch: 1229, Train Loss: 1.9415, Val Loss: 1.9777\n",
      "Epoch: 1230, Train Loss: 1.9494, Val Loss: 1.9643\n",
      "Epoch: 1231, Train Loss: 1.9574, Val Loss: 1.9413\n",
      "Epoch: 1232, Train Loss: 1.9491, Val Loss: 1.9189\n",
      "Epoch: 1233, Train Loss: 1.9659, Val Loss: 1.9857\n",
      "Epoch: 1234, Train Loss: 1.9624, Val Loss: 1.9349\n",
      "Epoch: 1235, Train Loss: 1.9481, Val Loss: 1.9428\n",
      "Epoch: 1236, Train Loss: 1.9553, Val Loss: 1.9672\n",
      "Epoch: 1237, Train Loss: 1.9644, Val Loss: 1.9445\n",
      "Epoch: 1238, Train Loss: 1.9450, Val Loss: 1.9844\n",
      "Epoch: 1239, Train Loss: 1.9736, Val Loss: 1.9887\n",
      "Epoch: 1240, Train Loss: 1.9807, Val Loss: 1.9822\n",
      "Epoch: 1241, Train Loss: 1.9490, Val Loss: 1.9588\n",
      "Epoch: 1242, Train Loss: 1.9500, Val Loss: 1.9781\n",
      "Epoch: 1243, Train Loss: 1.9585, Val Loss: 1.9866\n",
      "Epoch: 1244, Train Loss: 1.9602, Val Loss: 1.9558\n",
      "Epoch: 1245, Train Loss: 1.9462, Val Loss: 1.9555\n",
      "Epoch: 1246, Train Loss: 1.9528, Val Loss: 1.9715\n",
      "Epoch: 1247, Train Loss: 1.9532, Val Loss: 1.9864\n",
      "Epoch: 1248, Train Loss: 1.9925, Val Loss: 1.9559\n",
      "Epoch: 1249, Train Loss: 1.9594, Val Loss: 1.9670\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m vxb, vyb = get_batch(batch, \u001b[32m128\u001b[39m, val_data)\n\u001b[32m     21\u001b[39m vxb, vyb = vxb.to(device), vyb.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvxb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass (no gradient)\u001b[39;00m\n\u001b[32m     24\u001b[39m b, t, n = output.shape\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Compute validation loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mTransformerMini.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    100\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.mlp(x)\n\u001b[32m    103\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.logits(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m b, t, c = x.shape\n\u001b[32m     57\u001b[39m x_ln = F.layer_norm(x, (c,))  \u001b[38;5;66;03m# Correct normalization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m output = [\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ln\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads]\n\u001b[32m     59\u001b[39m output = torch.cat(output, dim=\u001b[32m2\u001b[39m)\n\u001b[32m     60\u001b[39m x_p = \u001b[38;5;28mself\u001b[39m.projection_weight(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NileshTN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m     attn_weights = scaled_scores\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     attn_weights = \u001b[43mscaled_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-inf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m attn_weights = F.softmax(attn_weights, dim=-\u001b[32m1\u001b[39m)  \n\u001b[32m     40\u001b[39m attn_weights = \u001b[38;5;28mself\u001b[39m.dropout(attn_weights)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    # Get training batch\n",
    "    xb, yb = get_batch(batch, context_lenth, train_data)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(xb)  # Forward pass\n",
    "    b, t, n = output.shape\n",
    "\n",
    "    # Compute training loss\n",
    "    loss = criterion(output.view(b * t, n), yb.view(b * t))  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vxb, vyb = get_batch(batch, 128, val_data)\n",
    "        vxb, vyb = vxb.to(device), vyb.to(device)\n",
    "\n",
    "        output = model(vxb)  # Forward pass (no gradient)\n",
    "        b, t, n = output.shape\n",
    "\n",
    "        # Compute validation loss\n",
    "        vloss = criterion(output.view(b * t, n), vyb.view(b * t))  \n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch: {i}, Train Loss: {loss.item():.4f}, Val Loss: {vloss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(model:TransformerMini, token_len):\n",
    "    token = torch.tensor([[0]]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(token_len):\n",
    "            out = model(token)\n",
    "            sm_out = F.softmax(out[:,-1,:], dim=-1)\n",
    "            predict = torch.multinomial(sm_out, num_samples=1)\n",
    "            token = torch.cat((token, predict), dim=1)[:, -(context_lenth-1):]\n",
    "            print(tokenizer.decode(predict[0].cpu().detach().tolist()), end='')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABULIAu herfor knoureembe I seaccomerereaught.\n",
      "\n",
      "LINGE:\n",
      "If thall slady see Made sair I thing fee the's a neass\n",
      "Hamel thesen foirscory muteth wothe pers es heir a joy fiter\n",
      "band therefuld of his mus to Bus erear theser laike so an crock\n",
      "On yousend; ust me seand all and you hour of trus ham,\n",
      "This is will goveredly weas bocith to be Band!\n",
      "\n",
      "DUKE OF YORK:\n",
      "Sill buson, home bethe to he'll hemselfe?\n",
      "\n",
      "Tho can-do wand nould you, murce you and him with he porse,\n",
      "Wore me and by warland wrefor nuth your of you.\n",
      "\n",
      "Shard your king thall worder be such you;\n",
      "And he lover hou moth, comarters letche aded murt.\n",
      "\n",
      "KING RICHARD II:\n",
      "And your of noy lood, kis, I was was him noth kind:\n",
      "the ve have vuse sajoy with.\n",
      "\n",
      "CORD OFIO:\n",
      "But of peas dieng in a wing you you,\n",
      "Sothen A she a bee He paid to sold to spot I what he fort,\n",
      "The the for stre? wher hy, sher nobery is barrest thall dime and weared\n",
      "Her forrdius she, shapur ther that of me?\n",
      "\n",
      "POLIXENRY Bordirselve ine somblid a cone appost? I hrue bron rear them do der lea"
     ]
    }
   ],
   "source": [
    "generator(model.eval(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/transformer_trained_model/trained_v0.0.3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
